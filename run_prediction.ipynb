{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shimi\\git\\workspace_python\\tme_imputation\\models\\lstm.py:2: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shimi\\git\\workspace_python\\tme_imputation\\models\\lstm.py:2: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6B550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6B550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6BEB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6BEB8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6B550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6B550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6BEB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6BEB8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6B550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6B550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6BEB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023ACEB6BEB8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from models import gcn, lstm\n",
    "from configs import *\n",
    "from utils import *\n",
    "import scipy.sparse\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "dataset = 'datasets/rppa'\n",
    "time_steps = 5\n",
    "train_ratio = 0.7\n",
    "batch_size=200\n",
    "gcn_layers=2\n",
    "hidden_dim=5\n",
    "hidden_size=6\n",
    "time_steps=5\n",
    "dropout_prob=0.\n",
    "learning_rate=0.001\n",
    "\n",
    "\n",
    "adjs, feats, train_idx, val_idx, test_idx = load_data(dataset, time_steps, train_ratio)\n",
    "\n",
    "num_node = adjs[0].shape[0]\n",
    "num_feat = feats[0].shape[1]\n",
    "\n",
    "for i in range(time_steps):\n",
    "    adjs[i] = sparse_to_tuple(scipy.sparse.coo_matrix(adjs[i]))\n",
    "#     feats[i] = sparse_to_tuple(scipy.sparse.coo_matrix(feats[i]))\n",
    "num_features_nonzeros = [x[1].shape for x in feats]\n",
    "\n",
    "# define placeholders of the input data \n",
    "phs = {\n",
    "        'adjs': [tf.sparse_placeholder(tf.float32, shape=(None, None), name=\"adjs\") for i in\n",
    "             range(time_steps)],\n",
    "        'feats': [tf.placeholder(tf.float32, shape=(None, num_feat), name=\"feats\") for _ in\n",
    "                 range(time_steps)],\n",
    "        'train_idx': tf.placeholder(tf.int32, shape=(None,), name=\"train_idx\"),\n",
    "        'val_idx': tf.placeholder(tf.int32, shape=(None,), name=\"val_idx\"),\n",
    "        'test_idx': tf.placeholder(tf.int32, shape=(None,), name=\"test_idx\"),\n",
    "        'sample_idx': tf.placeholder(tf.int32, shape=(batch_size,), name='batch_sample_idx'),\n",
    "        'dropout_prob': tf.placeholder_with_default(0., shape=()),\n",
    "        'num_features_nonzeros': [tf.placeholder(tf.int64) for i in range(time_steps)]\n",
    "        }\n",
    "\n",
    "# define the GCN model\n",
    "gcn_model = gcn.GraphConvLayer(time_steps = time_steps,\n",
    "                               gcn_layers=gcn_layers,\n",
    "                               input_dim=num_feat,\n",
    "                               hidden_dim=hidden_dim,\n",
    "                               output_dim=hidden_size,\n",
    "                               name='nn_fc1',\n",
    "                               num_features_nonzeros=phs['num_features_nonzeros'],\n",
    "                               act=tf.nn.relu,\n",
    "                               dropout_prob=phs['dropout_prob'],\n",
    "                               dropout=True)\n",
    "embeds_list = gcn_model(adjs=phs['adjs'],\n",
    "                    feats=phs['feats'],\n",
    "                    sparse=False)\n",
    "\n",
    "# prepare train data for the LSTM-based prediction model\n",
    "## replace all missing features at (time_steps-1) with GCN imputed features\n",
    "# embeds_list[time_steps-1] = tf.add(phs['feats'][time_steps-1], \n",
    "#                                    tf.multiply(phs['test_mask'][time_steps-1], embeds_list[time_steps-1]))\n",
    "## construct training samples for the prediction task\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = build_train_samples_imputation(embeds_list=embeds_list, \n",
    "                                                                     feats=phs['feats'], \n",
    "                                                                     train_idx=phs['train_idx'],\n",
    "                                                                     val_idx=phs['val_idx'],\n",
    "                                                                     test_idx=phs['test_idx'],\n",
    "                                                                     time_steps=time_steps)\n",
    "# define the bi-directional LSTM model\n",
    "lstm_model = lstm.BiLSTM(hidden_size=hidden_size,\n",
    "                         seq_len=time_steps-1,\n",
    "                         holders=phs)\n",
    "x_input_seq = tf.gather(x_train, phs['sample_idx'])\n",
    "y_input_seq_real = tf.gather(y_train, phs['sample_idx'])\n",
    "y_input_seq_pred = lstm_model(input_seq=x_input_seq)\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    # calculate the train mse and ad\n",
    "    train_mse = tf.losses.mean_squared_error(y_input_seq_real, y_input_seq_pred)\n",
    "    train_absolute_diff = tf.losses.absolute_difference(y_input_seq_real, y_input_seq_pred)\n",
    "    \n",
    "    # calculate the val mse and ad\n",
    "    val_input_seq_pred = lstm_model(input_seq=x_val)\n",
    "    val_mse = tf.losses.mean_squared_error(y_val, val_input_seq_pred)\n",
    "    val_absolute_diff = tf.losses.absolute_difference(y_val, val_input_seq_pred)\n",
    "    \n",
    "    # calculate the test mse and ad\n",
    "    test_input_seq_pred = lstm_model(input_seq=x_test)\n",
    "    test_mse = tf.losses.mean_squared_error(y_test, test_input_seq_pred)\n",
    "    test_absolute_diff = tf.losses.absolute_difference(y_test, test_input_seq_pred)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    opt_op = optimizer.minimize(train_mse)\n",
    "\n",
    "n_cpus = 8\n",
    "config = tf.ConfigProto(device_count={ \"CPU\": n_cpus},\n",
    "                            inter_op_parallelism_threads=n_cpus,\n",
    "                            intra_op_parallelism_threads=2)\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "feed_dict = {phs['train_idx']: train_idx,\n",
    "             phs['val_idx']: val_idx,\n",
    "             phs['test_idx']: test_idx,\n",
    "             phs['sample_idx']: None,\n",
    "             phs['dropout_prob']: dropout_prob}\n",
    "\n",
    "feed_dict.update({phs['adjs'][t]: adjs[t] for t in range(time_steps)})\n",
    "feed_dict.update({phs['feats'][t]: feats[t] for t in range(time_steps)})\n",
    "feed_dict.update({phs['num_features_nonzeros'][t]: num_features_nonzeros[t] for t in range(time_steps)})\n",
    "\n",
    "feed_dict_val = {phs['train_idx']: train_idx,\n",
    "                 phs['val_idx']: val_idx,\n",
    "                 phs['test_idx']: test_idx,\n",
    "                 phs['dropout_prob']: 0}\n",
    "\n",
    "feed_dict_val.update({phs['adjs'][t]: adjs[t] for t in range(time_steps)})\n",
    "feed_dict_val.update({phs['feats'][t]: feats[t] for t in range(time_steps)})\n",
    "feed_dict_val.update({phs['num_features_nonzeros'][t]: num_features_nonzeros[t] for t in range(time_steps)})\n",
    "\n",
    "\n",
    "\n",
    "def get_batch_idx(epoch):\n",
    "    s = batch_size * epoch\n",
    "    e = batch_size * (epoch + 1)\n",
    "    idx = []\n",
    "    for i in range(s,e):\n",
    "        idx.append(i%len(train_idx))\n",
    "    return idx\n",
    "\n",
    "save_step = 10\n",
    "t = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.23824 train_MSE= 0.23824 train_AD= 0.33978 val_MSE= 0.20709 val_AD= 0.31664 time= 1.32583\n",
      "Epoch: 0002 train_loss= 0.24079 train_MSE= 0.24079 train_AD= 0.34059 val_MSE= 0.20319 val_AD= 0.31332 time= 1.33780\n",
      "Epoch: 0003 train_loss= 0.23603 train_MSE= 0.23603 train_AD= 0.33695 val_MSE= 0.19932 val_AD= 0.31003 time= 1.34977\n",
      "Epoch: 0004 train_loss= 0.23193 train_MSE= 0.23193 train_AD= 0.33407 val_MSE= 0.19547 val_AD= 0.30671 time= 1.36075\n",
      "Epoch: 0005 train_loss= 0.22160 train_MSE= 0.22160 train_AD= 0.32753 val_MSE= 0.19163 val_AD= 0.30332 time= 1.37171\n",
      "Epoch: 0006 train_loss= 0.22266 train_MSE= 0.22266 train_AD= 0.32868 val_MSE= 0.18779 val_AD= 0.29985 time= 1.38369\n",
      "Epoch: 0007 train_loss= 0.21850 train_MSE= 0.21850 train_AD= 0.32535 val_MSE= 0.18397 val_AD= 0.29632 time= 1.39365\n",
      "Epoch: 0008 train_loss= 0.21355 train_MSE= 0.21355 train_AD= 0.32143 val_MSE= 0.18016 val_AD= 0.29272 time= 1.40365\n",
      "Epoch: 0009 train_loss= 0.20215 train_MSE= 0.20215 train_AD= 0.31349 val_MSE= 0.17644 val_AD= 0.28936 time= 1.41460\n",
      "Epoch: 0010 train_loss= 0.19917 train_MSE= 0.19917 train_AD= 0.30929 val_MSE= 0.17272 val_AD= 0.28594 time= 1.42557\n",
      "-------test_MSE= 0.18276 test_AD= 0.29414\n",
      "Epoch: 0011 train_loss= 0.19548 train_MSE= 0.19548 train_AD= 0.30702 val_MSE= 0.16899 val_AD= 0.28242 time= 1.54626\n",
      "Epoch: 0012 train_loss= 0.19351 train_MSE= 0.19351 train_AD= 0.30505 val_MSE= 0.16536 val_AD= 0.27885 time= 1.56021\n",
      "Epoch: 0013 train_loss= 0.18609 train_MSE= 0.18609 train_AD= 0.29733 val_MSE= 0.16175 val_AD= 0.27514 time= 1.57118\n",
      "Epoch: 0014 train_loss= 0.18393 train_MSE= 0.18393 train_AD= 0.29585 val_MSE= 0.15815 val_AD= 0.27131 time= 1.58215\n",
      "Epoch: 0015 train_loss= 0.17891 train_MSE= 0.17891 train_AD= 0.29122 val_MSE= 0.15460 val_AD= 0.26730 time= 1.59212\n",
      "Epoch: 0016 train_loss= 0.16966 train_MSE= 0.16966 train_AD= 0.28480 val_MSE= 0.15111 val_AD= 0.26328 time= 1.60210\n",
      "Epoch: 0017 train_loss= 0.17120 train_MSE= 0.17120 train_AD= 0.28603 val_MSE= 0.14777 val_AD= 0.25950 time= 1.61207\n",
      "Epoch: 0018 train_loss= 0.16888 train_MSE= 0.16888 train_AD= 0.28169 val_MSE= 0.14486 val_AD= 0.25657 time= 1.62306\n",
      "Epoch: 0019 train_loss= 0.16413 train_MSE= 0.16413 train_AD= 0.27809 val_MSE= 0.14236 val_AD= 0.25404 time= 1.63301\n",
      "Epoch: 0020 train_loss= 0.16345 train_MSE= 0.16345 train_AD= 0.27806 val_MSE= 0.14026 val_AD= 0.25168 time= 1.64299\n",
      "-------test_MSE= 0.15022 test_AD= 0.25842\n",
      "Epoch: 0021 train_loss= 0.16119 train_MSE= 0.16119 train_AD= 0.27545 val_MSE= 0.13863 val_AD= 0.24926 time= 1.65795\n",
      "Epoch: 0022 train_loss= 0.15810 train_MSE= 0.15810 train_AD= 0.27275 val_MSE= 0.13751 val_AD= 0.24680 time= 1.66792\n",
      "Epoch: 0023 train_loss= 0.15686 train_MSE= 0.15686 train_AD= 0.27190 val_MSE= 0.13691 val_AD= 0.24431 time= 1.68089\n",
      "Epoch: 0024 train_loss= 0.15239 train_MSE= 0.15239 train_AD= 0.26726 val_MSE= 0.13674 val_AD= 0.24358 time= 1.69186\n",
      "Epoch: 0025 train_loss= 0.15354 train_MSE= 0.15354 train_AD= 0.26860 val_MSE= 0.13688 val_AD= 0.24395 time= 1.70283\n",
      "Epoch: 0026 train_loss= 0.14700 train_MSE= 0.14700 train_AD= 0.26258 val_MSE= 0.13719 val_AD= 0.24433 time= 1.71280\n",
      "Epoch: 0027 train_loss= 0.15060 train_MSE= 0.15060 train_AD= 0.26708 val_MSE= 0.13748 val_AD= 0.24478 time= 1.72277\n",
      "Epoch: 0028 train_loss= 0.14988 train_MSE= 0.14988 train_AD= 0.26504 val_MSE= 0.13767 val_AD= 0.24493 time= 1.73275\n",
      "Epoch: 0029 train_loss= 0.14272 train_MSE= 0.14272 train_AD= 0.26410 val_MSE= 0.13765 val_AD= 0.24471 time= 1.74172\n",
      "Epoch: 0030 train_loss= 0.15004 train_MSE= 0.15004 train_AD= 0.26616 val_MSE= 0.13742 val_AD= 0.24413 time= 1.75469\n",
      "-------test_MSE= 0.13968 test_AD= 0.24514\n",
      "Epoch: 0031 train_loss= 0.14783 train_MSE= 0.14783 train_AD= 0.26410 val_MSE= 0.13698 val_AD= 0.24323 time= 1.77066\n",
      "Epoch: 0032 train_loss= 0.14794 train_MSE= 0.14794 train_AD= 0.26437 val_MSE= 0.13635 val_AD= 0.24205 time= 1.78261\n",
      "Epoch: 0033 train_loss= 0.14578 train_MSE= 0.14578 train_AD= 0.26091 val_MSE= 0.13560 val_AD= 0.24089 time= 1.79458\n",
      "Epoch: 0034 train_loss= 0.12654 train_MSE= 0.12654 train_AD= 0.25124 val_MSE= 0.13479 val_AD= 0.23959 time= 1.80656\n",
      "Epoch: 0035 train_loss= 0.13282 train_MSE= 0.13282 train_AD= 0.24949 val_MSE= 0.13398 val_AD= 0.23812 time= 1.81753\n",
      "Epoch: 0036 train_loss= 0.14163 train_MSE= 0.14163 train_AD= 0.25689 val_MSE= 0.13319 val_AD= 0.23650 time= 1.82750\n",
      "Epoch: 0037 train_loss= 0.14290 train_MSE= 0.14290 train_AD= 0.25766 val_MSE= 0.13248 val_AD= 0.23515 time= 1.83747\n",
      "Epoch: 0038 train_loss= 0.14010 train_MSE= 0.14010 train_AD= 0.25383 val_MSE= 0.13187 val_AD= 0.23377 time= 1.84744\n",
      "Epoch: 0039 train_loss= 0.13912 train_MSE= 0.13912 train_AD= 0.25218 val_MSE= 0.13136 val_AD= 0.23241 time= 1.85842\n",
      "Epoch: 0040 train_loss= 0.13804 train_MSE= 0.13804 train_AD= 0.25195 val_MSE= 0.13096 val_AD= 0.23167 time= 1.86838\n",
      "-------test_MSE= 0.13169 test_AD= 0.23336\n",
      "Epoch: 0041 train_loss= 0.13960 train_MSE= 0.13960 train_AD= 0.25355 val_MSE= 0.13063 val_AD= 0.23089 time= 1.88235\n",
      "Epoch: 0042 train_loss= 0.13932 train_MSE= 0.13932 train_AD= 0.25305 val_MSE= 0.13033 val_AD= 0.23038 time= 1.89332\n",
      "Epoch: 0043 train_loss= 0.13746 train_MSE= 0.13746 train_AD= 0.25122 val_MSE= 0.13004 val_AD= 0.22994 time= 1.90430\n",
      "Epoch: 0044 train_loss= 0.13380 train_MSE= 0.13380 train_AD= 0.24620 val_MSE= 0.12972 val_AD= 0.22931 time= 1.91326\n",
      "Epoch: 0045 train_loss= 0.13497 train_MSE= 0.13497 train_AD= 0.24853 val_MSE= 0.12938 val_AD= 0.22855 time= 1.92324\n",
      "Epoch: 0046 train_loss= 0.13659 train_MSE= 0.13659 train_AD= 0.24904 val_MSE= 0.12903 val_AD= 0.22765 time= 1.93520\n",
      "Epoch: 0047 train_loss= 0.13367 train_MSE= 0.13367 train_AD= 0.24532 val_MSE= 0.12867 val_AD= 0.22662 time= 1.94817\n",
      "Epoch: 0048 train_loss= 0.13297 train_MSE= 0.13297 train_AD= 0.24370 val_MSE= 0.12830 val_AD= 0.22551 time= 1.96313\n",
      "Epoch: 0049 train_loss= 0.13306 train_MSE= 0.13306 train_AD= 0.24451 val_MSE= 0.12794 val_AD= 0.22436 time= 1.97410\n",
      "Epoch: 0050 train_loss= 0.12707 train_MSE= 0.12707 train_AD= 0.24004 val_MSE= 0.12759 val_AD= 0.22313 time= 1.98507\n",
      "-------test_MSE= 0.12731 test_AD= 0.22548\n",
      "Epoch: 0051 train_loss= 0.13120 train_MSE= 0.13120 train_AD= 0.24365 val_MSE= 0.12726 val_AD= 0.22209 time= 2.00203\n",
      "Epoch: 0052 train_loss= 0.13236 train_MSE= 0.13236 train_AD= 0.24352 val_MSE= 0.12697 val_AD= 0.22138 time= 2.01100\n",
      "Epoch: 0053 train_loss= 0.13173 train_MSE= 0.13173 train_AD= 0.24352 val_MSE= 0.12673 val_AD= 0.22071 time= 2.02197\n",
      "Epoch: 0054 train_loss= 0.13066 train_MSE= 0.13066 train_AD= 0.24186 val_MSE= 0.12653 val_AD= 0.22005 time= 2.03095\n",
      "Epoch: 0055 train_loss= 0.13154 train_MSE= 0.13154 train_AD= 0.24323 val_MSE= 0.12638 val_AD= 0.21949 time= 2.04092\n",
      "Epoch: 0056 train_loss= 0.12969 train_MSE= 0.12969 train_AD= 0.24124 val_MSE= 0.12627 val_AD= 0.21911 time= 2.04990\n",
      "Epoch: 0057 train_loss= 0.13036 train_MSE= 0.13036 train_AD= 0.24200 val_MSE= 0.12618 val_AD= 0.21889 time= 2.05887\n",
      "Epoch: 0058 train_loss= 0.12890 train_MSE= 0.12890 train_AD= 0.24049 val_MSE= 0.12609 val_AD= 0.21870 time= 2.06885\n",
      "Epoch: 0059 train_loss= 0.12808 train_MSE= 0.12808 train_AD= 0.23931 val_MSE= 0.12599 val_AD= 0.21855 time= 2.07882\n",
      "Epoch: 0060 train_loss= 0.12382 train_MSE= 0.12382 train_AD= 0.23534 val_MSE= 0.12588 val_AD= 0.21825 time= 2.08979\n",
      "-------test_MSE= 0.12387 test_AD= 0.21821\n",
      "Epoch: 0061 train_loss= 0.12637 train_MSE= 0.12637 train_AD= 0.23710 val_MSE= 0.12574 val_AD= 0.21791 time= 2.10375\n",
      "Epoch: 0062 train_loss= 0.12697 train_MSE= 0.12697 train_AD= 0.23711 val_MSE= 0.12556 val_AD= 0.21749 time= 2.11473\n",
      "Epoch: 0063 train_loss= 0.12730 train_MSE= 0.12730 train_AD= 0.23839 val_MSE= 0.12535 val_AD= 0.21704 time= 2.12672\n",
      "Epoch: 0064 train_loss= 0.11919 train_MSE= 0.11919 train_AD= 0.23484 val_MSE= 0.12512 val_AD= 0.21655 time= 2.13766\n",
      "Epoch: 0065 train_loss= 0.12640 train_MSE= 0.12640 train_AD= 0.23692 val_MSE= 0.12488 val_AD= 0.21602 time= 2.14764\n",
      "Epoch: 0066 train_loss= 0.12547 train_MSE= 0.12547 train_AD= 0.23628 val_MSE= 0.12463 val_AD= 0.21544 time= 2.15761\n",
      "Epoch: 0067 train_loss= 0.12515 train_MSE= 0.12515 train_AD= 0.23489 val_MSE= 0.12438 val_AD= 0.21485 time= 2.16759\n",
      "Epoch: 0068 train_loss= 0.10756 train_MSE= 0.10756 train_AD= 0.22586 val_MSE= 0.12415 val_AD= 0.21428 time= 2.17756\n",
      "Epoch: 0069 train_loss= 0.11499 train_MSE= 0.11499 train_AD= 0.22520 val_MSE= 0.12395 val_AD= 0.21376 time= 2.18753\n",
      "Epoch: 0070 train_loss= 0.11991 train_MSE= 0.11991 train_AD= 0.22917 val_MSE= 0.12377 val_AD= 0.21339 time= 2.19750\n",
      "-------test_MSE= 0.12020 test_AD= 0.21149\n",
      "Epoch: 0071 train_loss= 0.12420 train_MSE= 0.12420 train_AD= 0.23333 val_MSE= 0.12360 val_AD= 0.21303 time= 2.21149\n",
      "Epoch: 0072 train_loss= 0.12361 train_MSE= 0.12361 train_AD= 0.23214 val_MSE= 0.12344 val_AD= 0.21271 time= 2.22244\n",
      "Epoch: 0073 train_loss= 0.12154 train_MSE= 0.12154 train_AD= 0.22904 val_MSE= 0.12331 val_AD= 0.21243 time= 2.23142\n",
      "Epoch: 0074 train_loss= 0.12159 train_MSE= 0.12159 train_AD= 0.22994 val_MSE= 0.12318 val_AD= 0.21216 time= 2.24039\n",
      "Epoch: 0075 train_loss= 0.12034 train_MSE= 0.12034 train_AD= 0.22825 val_MSE= 0.12308 val_AD= 0.21191 time= 2.25136\n",
      "Epoch: 0076 train_loss= 0.12204 train_MSE= 0.12204 train_AD= 0.23031 val_MSE= 0.12299 val_AD= 0.21167 time= 2.26233\n",
      "Epoch: 0077 train_loss= 0.12107 train_MSE= 0.12107 train_AD= 0.23022 val_MSE= 0.12290 val_AD= 0.21142 time= 2.27230\n",
      "Epoch: 0078 train_loss= 0.11903 train_MSE= 0.11903 train_AD= 0.22666 val_MSE= 0.12281 val_AD= 0.21115 time= 2.28128\n",
      "Epoch: 0079 train_loss= 0.11820 train_MSE= 0.11820 train_AD= 0.22621 val_MSE= 0.12271 val_AD= 0.21088 time= 2.29028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0080 train_loss= 0.11846 train_MSE= 0.11846 train_AD= 0.22567 val_MSE= 0.12260 val_AD= 0.21059 time= 2.30123\n",
      "-------test_MSE= 0.11756 test_AD= 0.20580\n",
      "Epoch: 0081 train_loss= 0.11863 train_MSE= 0.11863 train_AD= 0.22590 val_MSE= 0.12251 val_AD= 0.21032 time= 2.31519\n",
      "Epoch: 0082 train_loss= 0.11708 train_MSE= 0.11708 train_AD= 0.22250 val_MSE= 0.12241 val_AD= 0.21003 time= 2.32816\n",
      "Epoch: 0083 train_loss= 0.11873 train_MSE= 0.11873 train_AD= 0.22513 val_MSE= 0.12231 val_AD= 0.20974 time= 2.33912\n",
      "Epoch: 0084 train_loss= 0.11828 train_MSE= 0.11828 train_AD= 0.22458 val_MSE= 0.12222 val_AD= 0.20946 time= 2.35110\n",
      "Epoch: 0085 train_loss= 0.11219 train_MSE= 0.11219 train_AD= 0.22049 val_MSE= 0.12209 val_AD= 0.20914 time= 2.36210\n",
      "Epoch: 0086 train_loss= 0.11648 train_MSE= 0.11648 train_AD= 0.22316 val_MSE= 0.12196 val_AD= 0.20883 time= 2.37303\n",
      "Epoch: 0087 train_loss= 0.11810 train_MSE= 0.11810 train_AD= 0.22442 val_MSE= 0.12183 val_AD= 0.20850 time= 2.38500\n",
      "Epoch: 0088 train_loss= 0.11700 train_MSE= 0.11700 train_AD= 0.22309 val_MSE= 0.12166 val_AD= 0.20813 time= 2.39400\n",
      "Epoch: 0089 train_loss= 0.11722 train_MSE= 0.11722 train_AD= 0.22278 val_MSE= 0.12150 val_AD= 0.20776 time= 2.40495\n",
      "Epoch: 0090 train_loss= 0.11785 train_MSE= 0.11785 train_AD= 0.22415 val_MSE= 0.12134 val_AD= 0.20739 time= 2.41692\n",
      "-------test_MSE= 0.11562 test_AD= 0.20168\n",
      "Epoch: 0091 train_loss= 0.11618 train_MSE= 0.11618 train_AD= 0.22201 val_MSE= 0.12119 val_AD= 0.20704 time= 2.43088\n",
      "Epoch: 0092 train_loss= 0.11687 train_MSE= 0.11687 train_AD= 0.22250 val_MSE= 0.12104 val_AD= 0.20668 time= 2.44385\n",
      "Epoch: 0093 train_loss= 0.11463 train_MSE= 0.11463 train_AD= 0.21977 val_MSE= 0.12091 val_AD= 0.20633 time= 2.45382\n",
      "Epoch: 0094 train_loss= 0.11501 train_MSE= 0.11501 train_AD= 0.22014 val_MSE= 0.12078 val_AD= 0.20601 time= 2.46578\n",
      "Epoch: 0095 train_loss= 0.11183 train_MSE= 0.11183 train_AD= 0.21670 val_MSE= 0.12067 val_AD= 0.20574 time= 2.47776\n",
      "Epoch: 0096 train_loss= 0.11427 train_MSE= 0.11427 train_AD= 0.21879 val_MSE= 0.12058 val_AD= 0.20548 time= 2.48972\n",
      "Epoch: 0097 train_loss= 0.11546 train_MSE= 0.11546 train_AD= 0.22036 val_MSE= 0.12050 val_AD= 0.20525 time= 2.50269\n",
      "Epoch: 0098 train_loss= 0.10776 train_MSE= 0.10776 train_AD= 0.21770 val_MSE= 0.12043 val_AD= 0.20501 time= 2.51366\n",
      "Epoch: 0099 train_loss= 0.11484 train_MSE= 0.11484 train_AD= 0.21925 val_MSE= 0.12037 val_AD= 0.20481 time= 2.52563\n",
      "Epoch: 0100 train_loss= 0.11451 train_MSE= 0.11451 train_AD= 0.21958 val_MSE= 0.12034 val_AD= 0.20462 time= 2.53762\n",
      "-------test_MSE= 0.11409 test_AD= 0.19878\n",
      "Epoch: 0101 train_loss= 0.11469 train_MSE= 0.11469 train_AD= 0.21926 val_MSE= 0.12030 val_AD= 0.20441 time= 2.55256\n",
      "Epoch: 0102 train_loss= 0.09804 train_MSE= 0.09804 train_AD= 0.21141 val_MSE= 0.12029 val_AD= 0.20423 time= 2.56452\n",
      "Epoch: 0103 train_loss= 0.11186 train_MSE= 0.11186 train_AD= 0.21481 val_MSE= 0.12027 val_AD= 0.20404 time= 2.57649\n",
      "Epoch: 0104 train_loss= 0.10514 train_MSE= 0.10514 train_AD= 0.21032 val_MSE= 0.12026 val_AD= 0.20386 time= 2.58846\n",
      "Epoch: 0105 train_loss= 0.11252 train_MSE= 0.11252 train_AD= 0.21699 val_MSE= 0.12023 val_AD= 0.20368 time= 2.60042\n",
      "Epoch: 0106 train_loss= 0.11415 train_MSE= 0.11415 train_AD= 0.21838 val_MSE= 0.12018 val_AD= 0.20346 time= 2.61040\n",
      "Epoch: 0107 train_loss= 0.11315 train_MSE= 0.11315 train_AD= 0.21644 val_MSE= 0.12012 val_AD= 0.20321 time= 2.62237\n",
      "Epoch: 0108 train_loss= 0.11177 train_MSE= 0.11177 train_AD= 0.21494 val_MSE= 0.12002 val_AD= 0.20293 time= 2.63434\n",
      "Epoch: 0109 train_loss= 0.11084 train_MSE= 0.11084 train_AD= 0.21380 val_MSE= 0.11994 val_AD= 0.20266 time= 2.64431\n",
      "Epoch: 0110 train_loss= 0.11361 train_MSE= 0.11361 train_AD= 0.21780 val_MSE= 0.11985 val_AD= 0.20243 time= 2.65528\n",
      "-------test_MSE= 0.11314 test_AD= 0.19714\n",
      "Epoch: 0111 train_loss= 0.11327 train_MSE= 0.11327 train_AD= 0.21726 val_MSE= 0.11975 val_AD= 0.20223 time= 2.67223\n",
      "Epoch: 0112 train_loss= 0.10981 train_MSE= 0.10981 train_AD= 0.21303 val_MSE= 0.11964 val_AD= 0.20197 time= 2.68520\n",
      "Epoch: 0113 train_loss= 0.11162 train_MSE= 0.11162 train_AD= 0.21497 val_MSE= 0.11952 val_AD= 0.20170 time= 2.70016\n",
      "Epoch: 0114 train_loss= 0.10945 train_MSE= 0.10945 train_AD= 0.21308 val_MSE= 0.11940 val_AD= 0.20145 time= 2.71116\n",
      "Epoch: 0115 train_loss= 0.11128 train_MSE= 0.11128 train_AD= 0.21410 val_MSE= 0.11930 val_AD= 0.20128 time= 2.72411\n",
      "Epoch: 0116 train_loss= 0.11003 train_MSE= 0.11003 train_AD= 0.21142 val_MSE= 0.11923 val_AD= 0.20114 time= 2.73609\n",
      "Epoch: 0117 train_loss= 0.11143 train_MSE= 0.11143 train_AD= 0.21326 val_MSE= 0.11917 val_AD= 0.20105 time= 2.74604\n",
      "Epoch: 0118 train_loss= 0.11057 train_MSE= 0.11057 train_AD= 0.21234 val_MSE= 0.11914 val_AD= 0.20099 time= 2.75701\n",
      "Epoch: 0119 train_loss= 0.10504 train_MSE= 0.10504 train_AD= 0.20859 val_MSE= 0.11909 val_AD= 0.20094 time= 2.76701\n",
      "Epoch: 0120 train_loss= 0.11005 train_MSE= 0.11005 train_AD= 0.21293 val_MSE= 0.11905 val_AD= 0.20091 time= 2.77895\n",
      "-------test_MSE= 0.11258 test_AD= 0.19581\n",
      "Epoch: 0121 train_loss= 0.11111 train_MSE= 0.11111 train_AD= 0.21277 val_MSE= 0.11904 val_AD= 0.20090 time= 2.79690\n",
      "Epoch: 0122 train_loss= 0.11043 train_MSE= 0.11043 train_AD= 0.21252 val_MSE= 0.11901 val_AD= 0.20085 time= 2.80690\n",
      "Epoch: 0123 train_loss= 0.11134 train_MSE= 0.11134 train_AD= 0.21353 val_MSE= 0.11900 val_AD= 0.20080 time= 2.81785\n",
      "Epoch: 0124 train_loss= 0.11151 train_MSE= 0.11151 train_AD= 0.21353 val_MSE= 0.11899 val_AD= 0.20075 time= 2.82782\n",
      "Epoch: 0125 train_loss= 0.11014 train_MSE= 0.11014 train_AD= 0.21206 val_MSE= 0.11899 val_AD= 0.20069 time= 2.83980\n",
      "Epoch: 0126 train_loss= 0.11128 train_MSE= 0.11128 train_AD= 0.21372 val_MSE= 0.11900 val_AD= 0.20068 time= 2.85076\n",
      "Epoch: 0127 train_loss= 0.10923 train_MSE= 0.10923 train_AD= 0.21098 val_MSE= 0.11901 val_AD= 0.20066 time= 2.86373\n",
      "Epoch: 0128 train_loss= 0.11104 train_MSE= 0.11104 train_AD= 0.21348 val_MSE= 0.11901 val_AD= 0.20066 time= 2.87671\n",
      "Epoch: 0129 train_loss= 0.10502 train_MSE= 0.10502 train_AD= 0.20641 val_MSE= 0.11901 val_AD= 0.20066 time= 2.89166\n",
      "Epoch: 0130 train_loss= 0.10916 train_MSE= 0.10916 train_AD= 0.21064 val_MSE= 0.11901 val_AD= 0.20068 time= 2.90362\n",
      "-------test_MSE= 0.11234 test_AD= 0.19545\n",
      "Epoch: 0131 train_loss= 0.11016 train_MSE= 0.11016 train_AD= 0.21128 val_MSE= 0.11902 val_AD= 0.20072 time= 2.91957\n",
      "Epoch: 0132 train_loss= 0.10271 train_MSE= 0.10271 train_AD= 0.20982 val_MSE= 0.11903 val_AD= 0.20076 time= 2.93254\n",
      "Epoch: 0133 train_loss= 0.11048 train_MSE= 0.11048 train_AD= 0.21218 val_MSE= 0.11905 val_AD= 0.20081 time= 2.94550\n",
      "Epoch: 0134 train_loss= 0.10924 train_MSE= 0.10924 train_AD= 0.21131 val_MSE= 0.11909 val_AD= 0.20091 time= 2.95747\n",
      "Epoch: 0135 train_loss= 0.11065 train_MSE= 0.11065 train_AD= 0.21287 val_MSE= 0.11914 val_AD= 0.20102 time= 2.97243\n",
      "Epoch: 0136 train_loss= 0.10999 train_MSE= 0.10999 train_AD= 0.21113 val_MSE= 0.11920 val_AD= 0.20114 time= 2.98639\n",
      "Epoch: 0137 train_loss= 0.09179 train_MSE= 0.09179 train_AD= 0.20155 val_MSE= 0.11928 val_AD= 0.20125 time= 2.99736\n",
      "Epoch: 0138 train_loss= 0.10087 train_MSE= 0.10087 train_AD= 0.20262 val_MSE= 0.11937 val_AD= 0.20136 time= 3.01033\n",
      "Epoch: 0139 train_loss= 0.10852 train_MSE= 0.10852 train_AD= 0.21018 val_MSE= 0.11943 val_AD= 0.20146 time= 3.02429\n",
      "Epoch: 0140 train_loss= 0.11046 train_MSE= 0.11046 train_AD= 0.21246 val_MSE= 0.11946 val_AD= 0.20152 time= 3.03828\n",
      "-------test_MSE= 0.11222 test_AD= 0.19567\n",
      "Epoch: 0141 train_loss= 0.10962 train_MSE= 0.10962 train_AD= 0.21082 val_MSE= 0.11948 val_AD= 0.20156 time= 3.05527\n",
      "Epoch: 0142 train_loss= 0.10884 train_MSE= 0.10884 train_AD= 0.20959 val_MSE= 0.11949 val_AD= 0.20159 time= 3.06917\n",
      "Epoch: 0143 train_loss= 0.10675 train_MSE= 0.10675 train_AD= 0.20742 val_MSE= 0.11950 val_AD= 0.20163 time= 3.08014\n",
      "Epoch: 0144 train_loss= 0.10946 train_MSE= 0.10946 train_AD= 0.21078 val_MSE= 0.11949 val_AD= 0.20164 time= 3.09311\n",
      "Epoch: 0145 train_loss= 0.10991 train_MSE= 0.10991 train_AD= 0.21195 val_MSE= 0.11948 val_AD= 0.20162 time= 3.10508\n",
      "Epoch: 0146 train_loss= 0.10876 train_MSE= 0.10876 train_AD= 0.21058 val_MSE= 0.11946 val_AD= 0.20157 time= 3.11804\n",
      "Epoch: 0147 train_loss= 0.10675 train_MSE= 0.10675 train_AD= 0.20798 val_MSE= 0.11941 val_AD= 0.20146 time= 3.12901\n",
      "Epoch: 0148 train_loss= 0.10677 train_MSE= 0.10677 train_AD= 0.20870 val_MSE= 0.11935 val_AD= 0.20135 time= 3.13998\n",
      "Epoch: 0149 train_loss= 0.10920 train_MSE= 0.10920 train_AD= 0.21031 val_MSE= 0.11931 val_AD= 0.20126 time= 3.15195\n",
      "Epoch: 0150 train_loss= 0.10721 train_MSE= 0.10721 train_AD= 0.20750 val_MSE= 0.11930 val_AD= 0.20120 time= 3.16192\n",
      "-------test_MSE= 0.11197 test_AD= 0.19541\n",
      "Epoch: 0151 train_loss= 0.10717 train_MSE= 0.10717 train_AD= 0.20654 val_MSE= 0.11929 val_AD= 0.20118 time= 3.17489\n",
      "Epoch: 0152 train_loss= 0.10794 train_MSE= 0.10794 train_AD= 0.20840 val_MSE= 0.11931 val_AD= 0.20120 time= 3.18686\n",
      "Epoch: 0153 train_loss= 0.10243 train_MSE= 0.10243 train_AD= 0.20455 val_MSE= 0.11933 val_AD= 0.20123 time= 3.19783\n",
      "Epoch: 0154 train_loss= 0.10753 train_MSE= 0.10753 train_AD= 0.20882 val_MSE= 0.11934 val_AD= 0.20128 time= 3.20980\n",
      "Epoch: 0155 train_loss= 0.10872 train_MSE= 0.10872 train_AD= 0.20901 val_MSE= 0.11938 val_AD= 0.20137 time= 3.22077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0156 train_loss= 0.10843 train_MSE= 0.10843 train_AD= 0.20941 val_MSE= 0.11940 val_AD= 0.20143 time= 3.23174\n",
      "Epoch: 0157 train_loss= 0.10824 train_MSE= 0.10824 train_AD= 0.20884 val_MSE= 0.11943 val_AD= 0.20149 time= 3.24271\n",
      "Epoch: 0158 train_loss= 0.10913 train_MSE= 0.10913 train_AD= 0.20993 val_MSE= 0.11944 val_AD= 0.20154 time= 3.25468\n",
      "Epoch: 0159 train_loss= 0.10773 train_MSE= 0.10773 train_AD= 0.20851 val_MSE= 0.11947 val_AD= 0.20160 time= 3.26664\n",
      "Epoch: 0160 train_loss= 0.10880 train_MSE= 0.10880 train_AD= 0.20985 val_MSE= 0.11950 val_AD= 0.20165 time= 3.27762\n",
      "-------test_MSE= 0.11179 test_AD= 0.19561\n",
      "Epoch: 0161 train_loss= 0.10807 train_MSE= 0.10807 train_AD= 0.20908 val_MSE= 0.11950 val_AD= 0.20166 time= 3.29357\n",
      "Epoch: 0162 train_loss= 0.10778 train_MSE= 0.10778 train_AD= 0.20884 val_MSE= 0.11953 val_AD= 0.20168 time= 3.30457\n",
      "Epoch: 0163 train_loss= 0.10355 train_MSE= 0.10355 train_AD= 0.20473 val_MSE= 0.11952 val_AD= 0.20168 time= 3.31651\n",
      "Epoch: 0164 train_loss= 0.10663 train_MSE= 0.10663 train_AD= 0.20681 val_MSE= 0.11952 val_AD= 0.20168 time= 3.32748\n",
      "Epoch: 0165 train_loss= 0.10804 train_MSE= 0.10804 train_AD= 0.20798 val_MSE= 0.11951 val_AD= 0.20167 time= 3.34045\n",
      "Epoch: 0166 train_loss= 0.10845 train_MSE= 0.10845 train_AD= 0.20936 val_MSE= 0.11953 val_AD= 0.20168 time= 3.35042\n",
      "Epoch: 0167 train_loss= 0.10043 train_MSE= 0.10043 train_AD= 0.20643 val_MSE= 0.11954 val_AD= 0.20170 time= 3.36239\n",
      "Epoch: 0168 train_loss= 0.10823 train_MSE= 0.10823 train_AD= 0.20947 val_MSE= 0.11958 val_AD= 0.20173 time= 3.37436\n",
      "Epoch: 0169 train_loss= 0.10774 train_MSE= 0.10774 train_AD= 0.20906 val_MSE= 0.11962 val_AD= 0.20177 time= 3.38632\n",
      "Epoch: 0170 train_loss= 0.10827 train_MSE= 0.10827 train_AD= 0.20897 val_MSE= 0.11968 val_AD= 0.20181 time= 3.39829\n",
      "-------test_MSE= 0.11155 test_AD= 0.19549\n",
      "Epoch: 0171 train_loss= 0.09119 train_MSE= 0.09119 train_AD= 0.20008 val_MSE= 0.11975 val_AD= 0.20184 time= 3.41525\n",
      "Epoch: 0172 train_loss= 0.09996 train_MSE= 0.09996 train_AD= 0.20074 val_MSE= 0.11982 val_AD= 0.20185 time= 3.42721\n",
      "Epoch: 0173 train_loss= 0.10438 train_MSE= 0.10438 train_AD= 0.20496 val_MSE= 0.11987 val_AD= 0.20186 time= 3.43818\n",
      "Epoch: 0174 train_loss= 0.10841 train_MSE= 0.10841 train_AD= 0.20933 val_MSE= 0.11987 val_AD= 0.20184 time= 3.45115\n",
      "Epoch: 0175 train_loss= 0.10844 train_MSE= 0.10844 train_AD= 0.20966 val_MSE= 0.11986 val_AD= 0.20179 time= 3.46115\n",
      "Epoch: 0176 train_loss= 0.10722 train_MSE= 0.10722 train_AD= 0.20678 val_MSE= 0.11985 val_AD= 0.20174 time= 3.47110\n",
      "Epoch: 0177 train_loss= 0.10682 train_MSE= 0.10682 train_AD= 0.20719 val_MSE= 0.11982 val_AD= 0.20168 time= 3.48207\n",
      "Epoch: 0178 train_loss= 0.10528 train_MSE= 0.10528 train_AD= 0.20512 val_MSE= 0.11980 val_AD= 0.20164 time= 3.49404\n",
      "Epoch: 0179 train_loss= 0.10781 train_MSE= 0.10781 train_AD= 0.20847 val_MSE= 0.11977 val_AD= 0.20158 time= 3.50501\n",
      "Epoch: 0180 train_loss= 0.10730 train_MSE= 0.10730 train_AD= 0.20901 val_MSE= 0.11975 val_AD= 0.20149 time= 3.51697\n",
      "-------test_MSE= 0.11133 test_AD= 0.19512\n",
      "Epoch: 0181 train_loss= 0.10615 train_MSE= 0.10615 train_AD= 0.20677 val_MSE= 0.11971 val_AD= 0.20136 time= 3.53592\n",
      "Epoch: 0182 train_loss= 0.10557 train_MSE= 0.10557 train_AD= 0.20668 val_MSE= 0.11967 val_AD= 0.20123 time= 3.55091\n",
      "Epoch: 0183 train_loss= 0.10566 train_MSE= 0.10566 train_AD= 0.20554 val_MSE= 0.11961 val_AD= 0.20112 time= 3.56385\n",
      "Epoch: 0184 train_loss= 0.10638 train_MSE= 0.10638 train_AD= 0.20696 val_MSE= 0.11959 val_AD= 0.20105 time= 3.57681\n",
      "Epoch: 0185 train_loss= 0.10482 train_MSE= 0.10482 train_AD= 0.20311 val_MSE= 0.11957 val_AD= 0.20099 time= 3.58978\n",
      "Epoch: 0186 train_loss= 0.10693 train_MSE= 0.10693 train_AD= 0.20696 val_MSE= 0.11955 val_AD= 0.20099 time= 3.60075\n",
      "Epoch: 0187 train_loss= 0.10701 train_MSE= 0.10701 train_AD= 0.20700 val_MSE= 0.11957 val_AD= 0.20101 time= 3.61272\n",
      "Epoch: 0188 train_loss= 0.10076 train_MSE= 0.10076 train_AD= 0.20262 val_MSE= 0.11955 val_AD= 0.20101 time= 3.62568\n",
      "Epoch: 0189 train_loss= 0.10563 train_MSE= 0.10563 train_AD= 0.20552 val_MSE= 0.11953 val_AD= 0.20105 time= 3.63765\n",
      "Epoch: 0190 train_loss= 0.10732 train_MSE= 0.10732 train_AD= 0.20758 val_MSE= 0.11952 val_AD= 0.20110 time= 3.64962\n",
      "-------test_MSE= 0.11113 test_AD= 0.19485\n",
      "Epoch: 0191 train_loss= 0.10631 train_MSE= 0.10631 train_AD= 0.20661 val_MSE= 0.11946 val_AD= 0.20108 time= 3.66757\n",
      "Epoch: 0192 train_loss= 0.10721 train_MSE= 0.10721 train_AD= 0.20725 val_MSE= 0.11942 val_AD= 0.20108 time= 3.68453\n",
      "Epoch: 0193 train_loss= 0.10747 train_MSE= 0.10747 train_AD= 0.20788 val_MSE= 0.11939 val_AD= 0.20108 time= 3.69749\n",
      "Epoch: 0194 train_loss= 0.10619 train_MSE= 0.10619 train_AD= 0.20651 val_MSE= 0.11938 val_AD= 0.20109 time= 3.70946\n",
      "Epoch: 0195 train_loss= 0.10710 train_MSE= 0.10710 train_AD= 0.20733 val_MSE= 0.11935 val_AD= 0.20107 time= 3.72145\n",
      "Epoch: 0196 train_loss= 0.10514 train_MSE= 0.10514 train_AD= 0.20528 val_MSE= 0.11936 val_AD= 0.20107 time= 3.73339\n",
      "Epoch: 0197 train_loss= 0.10582 train_MSE= 0.10582 train_AD= 0.20616 val_MSE= 0.11933 val_AD= 0.20105 time= 3.74437\n",
      "Epoch: 0198 train_loss= 0.10263 train_MSE= 0.10263 train_AD= 0.20258 val_MSE= 0.11933 val_AD= 0.20103 time= 3.75633\n",
      "Epoch: 0199 train_loss= 0.10539 train_MSE= 0.10539 train_AD= 0.20511 val_MSE= 0.11932 val_AD= 0.20102 time= 3.76731\n",
      "Epoch: 0200 train_loss= 0.10678 train_MSE= 0.10678 train_AD= 0.20705 val_MSE= 0.11935 val_AD= 0.20101 time= 3.77827\n",
      "-------test_MSE= 0.11089 test_AD= 0.19460\n",
      "Epoch: 0201 train_loss= 0.09893 train_MSE= 0.09893 train_AD= 0.20432 val_MSE= 0.11936 val_AD= 0.20101 time= 3.79423\n",
      "Epoch: 0202 train_loss= 0.10650 train_MSE= 0.10650 train_AD= 0.20688 val_MSE= 0.11938 val_AD= 0.20103 time= 3.80820\n",
      "Epoch: 0203 train_loss= 0.10629 train_MSE= 0.10629 train_AD= 0.20736 val_MSE= 0.11942 val_AD= 0.20106 time= 3.82116\n",
      "Epoch: 0204 train_loss= 0.10683 train_MSE= 0.10683 train_AD= 0.20751 val_MSE= 0.11946 val_AD= 0.20106 time= 3.83113\n",
      "Epoch: 0205 train_loss= 0.09040 train_MSE= 0.09040 train_AD= 0.19936 val_MSE= 0.11952 val_AD= 0.20109 time= 3.84310\n",
      "Epoch: 0206 train_loss= 0.10418 train_MSE= 0.10418 train_AD= 0.20317 val_MSE= 0.11956 val_AD= 0.20107 time= 3.85608\n",
      "Epoch: 0207 train_loss= 0.09829 train_MSE= 0.09829 train_AD= 0.19923 val_MSE= 0.11960 val_AD= 0.20107 time= 3.86803\n",
      "Epoch: 0208 train_loss= 0.10543 train_MSE= 0.10543 train_AD= 0.20617 val_MSE= 0.11959 val_AD= 0.20106 time= 3.88000\n",
      "Epoch: 0209 train_loss= 0.10685 train_MSE= 0.10685 train_AD= 0.20747 val_MSE= 0.11956 val_AD= 0.20101 time= 3.89297\n",
      "Epoch: 0210 train_loss= 0.10618 train_MSE= 0.10618 train_AD= 0.20588 val_MSE= 0.11952 val_AD= 0.20095 time= 3.90597\n",
      "-------test_MSE= 0.11077 test_AD= 0.19464\n",
      "Epoch: 0211 train_loss= 0.10505 train_MSE= 0.10505 train_AD= 0.20459 val_MSE= 0.11945 val_AD= 0.20089 time= 3.92289\n",
      "Epoch: 0212 train_loss= 0.10389 train_MSE= 0.10389 train_AD= 0.20339 val_MSE= 0.11938 val_AD= 0.20086 time= 3.93585\n",
      "Epoch: 0213 train_loss= 0.10662 train_MSE= 0.10662 train_AD= 0.20725 val_MSE= 0.11932 val_AD= 0.20082 time= 3.95181\n",
      "Epoch: 0214 train_loss= 0.10663 train_MSE= 0.10663 train_AD= 0.20740 val_MSE= 0.11926 val_AD= 0.20077 time= 3.96777\n",
      "Epoch: 0215 train_loss= 0.10371 train_MSE= 0.10371 train_AD= 0.20381 val_MSE= 0.11919 val_AD= 0.20064 time= 3.97877\n",
      "Epoch: 0216 train_loss= 0.10558 train_MSE= 0.10558 train_AD= 0.20618 val_MSE= 0.11913 val_AD= 0.20051 time= 3.99170\n",
      "Epoch: 0217 train_loss= 0.10327 train_MSE= 0.10327 train_AD= 0.20393 val_MSE= 0.11904 val_AD= 0.20038 time= 4.00467\n",
      "Epoch: 0218 train_loss= 0.10514 train_MSE= 0.10514 train_AD= 0.20510 val_MSE= 0.11897 val_AD= 0.20030 time= 4.01564\n",
      "Epoch: 0219 train_loss= 0.10385 train_MSE= 0.10385 train_AD= 0.20254 val_MSE= 0.11891 val_AD= 0.20024 time= 4.02860\n",
      "Epoch: 0220 train_loss= 0.10557 train_MSE= 0.10557 train_AD= 0.20507 val_MSE= 0.11886 val_AD= 0.20022 time= 4.04257\n",
      "-------test_MSE= 0.11061 test_AD= 0.19431\n",
      "Epoch: 0221 train_loss= 0.10492 train_MSE= 0.10492 train_AD= 0.20449 val_MSE= 0.11883 val_AD= 0.20022 time= 4.05952\n",
      "Epoch: 0222 train_loss= 0.09939 train_MSE= 0.09939 train_AD= 0.20061 val_MSE= 0.11875 val_AD= 0.20022 time= 4.07149\n",
      "Epoch: 0223 train_loss= 0.10462 train_MSE= 0.10462 train_AD= 0.20506 val_MSE= 0.11866 val_AD= 0.20024 time= 4.08446\n",
      "Epoch: 0224 train_loss= 0.10559 train_MSE= 0.10559 train_AD= 0.20509 val_MSE= 0.11860 val_AD= 0.20027 time= 4.09446\n",
      "Epoch: 0225 train_loss= 0.10484 train_MSE= 0.10484 train_AD= 0.20476 val_MSE= 0.11851 val_AD= 0.20027 time= 4.10640\n",
      "Epoch: 0226 train_loss= 0.10594 train_MSE= 0.10594 train_AD= 0.20612 val_MSE= 0.11845 val_AD= 0.20028 time= 4.11737\n",
      "Epoch: 0227 train_loss= 0.10611 train_MSE= 0.10611 train_AD= 0.20618 val_MSE= 0.11839 val_AD= 0.20030 time= 4.12934\n",
      "Epoch: 0228 train_loss= 0.10471 train_MSE= 0.10471 train_AD= 0.20439 val_MSE= 0.11836 val_AD= 0.20032 time= 4.13931\n",
      "Epoch: 0229 train_loss= 0.10596 train_MSE= 0.10596 train_AD= 0.20652 val_MSE= 0.11833 val_AD= 0.20033 time= 4.15028\n",
      "Epoch: 0230 train_loss= 0.10405 train_MSE= 0.10405 train_AD= 0.20376 val_MSE= 0.11831 val_AD= 0.20032 time= 4.16125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------test_MSE= 0.11049 test_AD= 0.19463\n",
      "Epoch: 0231 train_loss= 0.10579 train_MSE= 0.10579 train_AD= 0.20640 val_MSE= 0.11828 val_AD= 0.20032 time= 4.17821\n",
      "Epoch: 0232 train_loss= 0.10019 train_MSE= 0.10019 train_AD= 0.19971 val_MSE= 0.11824 val_AD= 0.20032 time= 4.19117\n",
      "Epoch: 0233 train_loss= 0.10421 train_MSE= 0.10421 train_AD= 0.20402 val_MSE= 0.11819 val_AD= 0.20029 time= 4.20414\n",
      "Epoch: 0234 train_loss= 0.10518 train_MSE= 0.10518 train_AD= 0.20470 val_MSE= 0.11816 val_AD= 0.20026 time= 4.21710\n",
      "Epoch: 0235 train_loss= 0.09782 train_MSE= 0.09782 train_AD= 0.20323 val_MSE= 0.11810 val_AD= 0.20021 time= 4.22907\n",
      "Epoch: 0236 train_loss= 0.10554 train_MSE= 0.10554 train_AD= 0.20577 val_MSE= 0.11807 val_AD= 0.20017 time= 4.24203\n",
      "Epoch: 0237 train_loss= 0.10428 train_MSE= 0.10428 train_AD= 0.20471 val_MSE= 0.11805 val_AD= 0.20015 time= 4.25301\n",
      "Epoch: 0238 train_loss= 0.10596 train_MSE= 0.10596 train_AD= 0.20669 val_MSE= 0.11806 val_AD= 0.20013 time= 4.26198\n",
      "Epoch: 0239 train_loss= 0.10523 train_MSE= 0.10523 train_AD= 0.20502 val_MSE= 0.11808 val_AD= 0.20011 time= 4.27395\n",
      "Epoch: 0240 train_loss= 0.08722 train_MSE= 0.08722 train_AD= 0.19533 val_MSE= 0.11811 val_AD= 0.20010 time= 4.28592\n",
      "-------test_MSE= 0.11045 test_AD= 0.19470\n",
      "Epoch: 0241 train_loss= 0.09636 train_MSE= 0.09636 train_AD= 0.19656 val_MSE= 0.11815 val_AD= 0.20011 time= 4.30287\n",
      "Epoch: 0242 train_loss= 0.10409 train_MSE= 0.10409 train_AD= 0.20439 val_MSE= 0.11815 val_AD= 0.20011 time= 4.31484\n",
      "Epoch: 0243 train_loss= 0.10579 train_MSE= 0.10579 train_AD= 0.20644 val_MSE= 0.11811 val_AD= 0.20010 time= 4.32880\n",
      "Epoch: 0244 train_loss= 0.10508 train_MSE= 0.10508 train_AD= 0.20505 val_MSE= 0.11805 val_AD= 0.20009 time= 4.33978\n",
      "Epoch: 0245 train_loss= 0.10442 train_MSE= 0.10442 train_AD= 0.20388 val_MSE= 0.11799 val_AD= 0.20007 time= 4.35174\n",
      "Epoch: 0246 train_loss= 0.10252 train_MSE= 0.10252 train_AD= 0.20176 val_MSE= 0.11792 val_AD= 0.20008 time= 4.36271\n",
      "Epoch: 0247 train_loss= 0.10488 train_MSE= 0.10488 train_AD= 0.20493 val_MSE= 0.11784 val_AD= 0.20006 time= 4.37468\n",
      "Epoch: 0248 train_loss= 0.10537 train_MSE= 0.10537 train_AD= 0.20612 val_MSE= 0.11776 val_AD= 0.20003 time= 4.38765\n",
      "Epoch: 0249 train_loss= 0.10438 train_MSE= 0.10438 train_AD= 0.20513 val_MSE= 0.11768 val_AD= 0.19995 time= 4.40161\n",
      "Epoch: 0250 train_loss= 0.10283 train_MSE= 0.10283 train_AD= 0.20295 val_MSE= 0.11759 val_AD= 0.19980 time= 4.41459\n",
      "-------test_MSE= 0.11037 test_AD= 0.19474\n",
      "Epoch: 0251 train_loss= 0.10240 train_MSE= 0.10240 train_AD= 0.20304 val_MSE= 0.11747 val_AD= 0.19964 time= 4.43053\n",
      "Epoch: 0252 train_loss= 0.10480 train_MSE= 0.10480 train_AD= 0.20477 val_MSE= 0.11737 val_AD= 0.19960 time= 4.44250\n",
      "Epoch: 0253 train_loss= 0.10290 train_MSE= 0.10290 train_AD= 0.20211 val_MSE= 0.11728 val_AD= 0.19957 time= 4.45347\n",
      "Epoch: 0254 train_loss= 0.10287 train_MSE= 0.10287 train_AD= 0.20126 val_MSE= 0.11719 val_AD= 0.19954 time= 4.46216\n",
      "Epoch: 0255 train_loss= 0.10373 train_MSE= 0.10373 train_AD= 0.20320 val_MSE= 0.11712 val_AD= 0.19954 time= 4.47313\n",
      "Epoch: 0256 train_loss= 0.09836 train_MSE= 0.09836 train_AD= 0.19959 val_MSE= 0.11704 val_AD= 0.19955 time= 4.48310\n",
      "Epoch: 0257 train_loss= 0.10351 train_MSE= 0.10351 train_AD= 0.20396 val_MSE= 0.11695 val_AD= 0.19958 time= 4.49208\n",
      "Epoch: 0258 train_loss= 0.10463 train_MSE= 0.10463 train_AD= 0.20425 val_MSE= 0.11689 val_AD= 0.19963 time= 4.50306\n",
      "Epoch: 0259 train_loss= 0.10416 train_MSE= 0.10416 train_AD= 0.20417 val_MSE= 0.11682 val_AD= 0.19967 time= 4.51502\n",
      "Epoch: 0260 train_loss= 0.10407 train_MSE= 0.10407 train_AD= 0.20388 val_MSE= 0.11679 val_AD= 0.19972 time= 4.52700\n",
      "-------test_MSE= 0.11035 test_AD= 0.19478\n",
      "Epoch: 0261 train_loss= 0.10496 train_MSE= 0.10496 train_AD= 0.20501 val_MSE= 0.11676 val_AD= 0.19978 time= 4.54394\n",
      "Epoch: 0262 train_loss= 0.10363 train_MSE= 0.10363 train_AD= 0.20358 val_MSE= 0.11676 val_AD= 0.19984 time= 4.55591\n",
      "Epoch: 0263 train_loss= 0.10466 train_MSE= 0.10466 train_AD= 0.20492 val_MSE= 0.11677 val_AD= 0.19990 time= 4.56887\n",
      "Epoch: 0264 train_loss= 0.10393 train_MSE= 0.10393 train_AD= 0.20409 val_MSE= 0.11677 val_AD= 0.19993 time= 4.57885\n",
      "Epoch: 0265 train_loss= 0.10374 train_MSE= 0.10374 train_AD= 0.20401 val_MSE= 0.11678 val_AD= 0.19995 time= 4.59081\n",
      "Epoch: 0266 train_loss= 0.09970 train_MSE= 0.09970 train_AD= 0.19993 val_MSE= 0.11676 val_AD= 0.19990 time= 4.60278\n",
      "Epoch: 0267 train_loss= 0.10272 train_MSE= 0.10272 train_AD= 0.20237 val_MSE= 0.11674 val_AD= 0.19985 time= 4.61575\n",
      "Epoch: 0268 train_loss= 0.10409 train_MSE= 0.10409 train_AD= 0.20345 val_MSE= 0.11670 val_AD= 0.19979 time= 4.62871\n",
      "Epoch: 0269 train_loss= 0.10450 train_MSE= 0.10450 train_AD= 0.20481 val_MSE= 0.11669 val_AD= 0.19975 time= 4.64268\n",
      "Epoch: 0270 train_loss= 0.09657 train_MSE= 0.09657 train_AD= 0.20203 val_MSE= 0.11666 val_AD= 0.19973 time= 4.65863\n",
      "-------test_MSE= 0.11035 test_AD= 0.19521\n",
      "Epoch: 0271 train_loss= 0.10423 train_MSE= 0.10423 train_AD= 0.20469 val_MSE= 0.11663 val_AD= 0.19972 time= 4.67758\n",
      "Epoch: 0272 train_loss= 0.10379 train_MSE= 0.10379 train_AD= 0.20456 val_MSE= 0.11663 val_AD= 0.19973 time= 4.68855\n",
      "Epoch: 0273 train_loss= 0.10439 train_MSE= 0.10439 train_AD= 0.20461 val_MSE= 0.11665 val_AD= 0.19977 time= 4.69952\n",
      "Epoch: 0274 train_loss= 0.08737 train_MSE= 0.08737 train_AD= 0.19573 val_MSE= 0.11667 val_AD= 0.19982 time= 4.71049\n",
      "Epoch: 0275 train_loss= 0.09623 train_MSE= 0.09623 train_AD= 0.19651 val_MSE= 0.11673 val_AD= 0.19990 time= 4.72247\n",
      "Epoch: 0276 train_loss= 0.10077 train_MSE= 0.10077 train_AD= 0.20079 val_MSE= 0.11674 val_AD= 0.19999 time= 4.73343\n",
      "Epoch: 0277 train_loss= 0.10457 train_MSE= 0.10457 train_AD= 0.20522 val_MSE= 0.11672 val_AD= 0.20006 time= 4.74540\n",
      "Epoch: 0278 train_loss= 0.10459 train_MSE= 0.10459 train_AD= 0.20542 val_MSE= 0.11668 val_AD= 0.20010 time= 4.75837\n",
      "Epoch: 0279 train_loss= 0.10355 train_MSE= 0.10355 train_AD= 0.20285 val_MSE= 0.11666 val_AD= 0.20013 time= 4.77134\n",
      "Epoch: 0280 train_loss= 0.10319 train_MSE= 0.10319 train_AD= 0.20323 val_MSE= 0.11661 val_AD= 0.20013 time= 4.78430\n",
      "-------test_MSE= 0.11041 test_AD= 0.19563\n",
      "Epoch: 0281 train_loss= 0.10165 train_MSE= 0.10165 train_AD= 0.20109 val_MSE= 0.11655 val_AD= 0.20014 time= 4.80424\n",
      "Epoch: 0282 train_loss= 0.10401 train_MSE= 0.10401 train_AD= 0.20442 val_MSE= 0.11648 val_AD= 0.20013 time= 4.81920\n",
      "Epoch: 0283 train_loss= 0.10351 train_MSE= 0.10351 train_AD= 0.20497 val_MSE= 0.11643 val_AD= 0.20009 time= 4.83317\n",
      "Epoch: 0284 train_loss= 0.10281 train_MSE= 0.10281 train_AD= 0.20327 val_MSE= 0.11636 val_AD= 0.20004 time= 4.84614\n",
      "Epoch: 0285 train_loss= 0.10190 train_MSE= 0.10190 train_AD= 0.20280 val_MSE= 0.11628 val_AD= 0.20001 time= 4.85910\n",
      "Epoch: 0286 train_loss= 0.10202 train_MSE= 0.10202 train_AD= 0.20156 val_MSE= 0.11618 val_AD= 0.19998 time= 4.87107\n",
      "Epoch: 0287 train_loss= 0.10271 train_MSE= 0.10271 train_AD= 0.20307 val_MSE= 0.11610 val_AD= 0.19997 time= 4.88604\n",
      "Epoch: 0288 train_loss= 0.10119 train_MSE= 0.10119 train_AD= 0.19935 val_MSE= 0.11601 val_AD= 0.19996 time= 4.89600\n",
      "Epoch: 0289 train_loss= 0.10335 train_MSE= 0.10335 train_AD= 0.20323 val_MSE= 0.11593 val_AD= 0.19995 time= 4.90600\n",
      "Epoch: 0290 train_loss= 0.10340 train_MSE= 0.10340 train_AD= 0.20346 val_MSE= 0.11589 val_AD= 0.19998 time= 4.91694\n",
      "-------test_MSE= 0.11040 test_AD= 0.19571\n",
      "Epoch: 0291 train_loss= 0.09731 train_MSE= 0.09731 train_AD= 0.19900 val_MSE= 0.11579 val_AD= 0.19999 time= 4.93290\n",
      "Epoch: 0292 train_loss= 0.10228 train_MSE= 0.10228 train_AD= 0.20214 val_MSE= 0.11570 val_AD= 0.20005 time= 4.94488\n",
      "Epoch: 0293 train_loss= 0.10374 train_MSE= 0.10374 train_AD= 0.20400 val_MSE= 0.11563 val_AD= 0.20012 time= 4.95985\n",
      "Epoch: 0294 train_loss= 0.10270 train_MSE= 0.10270 train_AD= 0.20299 val_MSE= 0.11555 val_AD= 0.20015 time= 4.97578\n",
      "Epoch: 0295 train_loss= 0.10366 train_MSE= 0.10366 train_AD= 0.20387 val_MSE= 0.11551 val_AD= 0.20022 time= 4.98775\n",
      "Epoch: 0296 train_loss= 0.10386 train_MSE= 0.10386 train_AD= 0.20421 val_MSE= 0.11552 val_AD= 0.20030 time= 4.99675\n",
      "Epoch: 0297 train_loss= 0.10265 train_MSE= 0.10265 train_AD= 0.20304 val_MSE= 0.11554 val_AD= 0.20040 time= 5.00670\n",
      "Epoch: 0298 train_loss= 0.10357 train_MSE= 0.10357 train_AD= 0.20385 val_MSE= 0.11556 val_AD= 0.20046 time= 5.01767\n",
      "Epoch: 0299 train_loss= 0.10159 train_MSE= 0.10159 train_AD= 0.20165 val_MSE= 0.11560 val_AD= 0.20052 time= 5.02764\n",
      "Epoch: 0300 train_loss= 0.10241 train_MSE= 0.10241 train_AD= 0.20284 val_MSE= 0.11558 val_AD= 0.20049 time= 5.03862\n",
      "-------test_MSE= 0.11045 test_AD= 0.19639\n",
      "Epoch: 0301 train_loss= 0.09940 train_MSE= 0.09940 train_AD= 0.19954 val_MSE= 0.11558 val_AD= 0.20046 time= 5.05558\n",
      "Epoch: 0302 train_loss= 0.10196 train_MSE= 0.10196 train_AD= 0.20188 val_MSE= 0.11555 val_AD= 0.20042 time= 5.06754\n",
      "Epoch: 0303 train_loss= 0.10334 train_MSE= 0.10334 train_AD= 0.20369 val_MSE= 0.11555 val_AD= 0.20040 time= 5.07754\n",
      "Epoch: 0304 train_loss= 0.09568 train_MSE= 0.09568 train_AD= 0.20110 val_MSE= 0.11553 val_AD= 0.20039 time= 5.08948\n",
      "Epoch: 0305 train_loss= 0.10306 train_MSE= 0.10306 train_AD= 0.20363 val_MSE= 0.11553 val_AD= 0.20041 time= 5.10145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0306 train_loss= 0.10284 train_MSE= 0.10284 train_AD= 0.20400 val_MSE= 0.11553 val_AD= 0.20050 time= 5.11242\n",
      "Epoch: 0307 train_loss= 0.10345 train_MSE= 0.10345 train_AD= 0.20437 val_MSE= 0.11556 val_AD= 0.20060 time= 5.12538\n",
      "Epoch: 0308 train_loss= 0.08705 train_MSE= 0.08705 train_AD= 0.19618 val_MSE= 0.11560 val_AD= 0.20071 time= 5.13835\n",
      "Epoch: 0309 train_loss= 0.10077 train_MSE= 0.10077 train_AD= 0.19995 val_MSE= 0.11565 val_AD= 0.20080 time= 5.15332\n",
      "Epoch: 0310 train_loss= 0.09510 train_MSE= 0.09510 train_AD= 0.19621 val_MSE= 0.11569 val_AD= 0.20089 time= 5.16628\n",
      "-------test_MSE= 0.11052 test_AD= 0.19685\n",
      "Epoch: 0311 train_loss= 0.10227 train_MSE= 0.10227 train_AD= 0.20333 val_MSE= 0.11568 val_AD= 0.20096 time= 5.18523\n",
      "Epoch: 0312 train_loss= 0.10348 train_MSE= 0.10348 train_AD= 0.20443 val_MSE= 0.11565 val_AD= 0.20100 time= 5.19819\n",
      "Epoch: 0313 train_loss= 0.10291 train_MSE= 0.10291 train_AD= 0.20310 val_MSE= 0.11561 val_AD= 0.20101 time= 5.21016\n",
      "Epoch: 0314 train_loss= 0.10190 train_MSE= 0.10190 train_AD= 0.20175 val_MSE= 0.11555 val_AD= 0.20097 time= 5.22113\n",
      "Epoch: 0315 train_loss= 0.10069 train_MSE= 0.10069 train_AD= 0.20054 val_MSE= 0.11548 val_AD= 0.20097 time= 5.23411\n",
      "Epoch: 0316 train_loss= 0.10322 train_MSE= 0.10322 train_AD= 0.20418 val_MSE= 0.11544 val_AD= 0.20098 time= 5.24606\n",
      "Epoch: 0317 train_loss= 0.10332 train_MSE= 0.10332 train_AD= 0.20452 val_MSE= 0.11538 val_AD= 0.20096 time= 5.25803\n",
      "Epoch: 0318 train_loss= 0.10073 train_MSE= 0.10073 train_AD= 0.20122 val_MSE= 0.11533 val_AD= 0.20092 time= 5.27099\n",
      "Epoch: 0319 train_loss= 0.10232 train_MSE= 0.10232 train_AD= 0.20338 val_MSE= 0.11527 val_AD= 0.20089 time= 5.28496\n",
      "Epoch: 0320 train_loss= 0.10010 train_MSE= 0.10010 train_AD= 0.20108 val_MSE= 0.11519 val_AD= 0.20089 time= 5.29992\n",
      "-------test_MSE= 0.11051 test_AD= 0.19717\n",
      "Epoch: 0321 train_loss= 0.10189 train_MSE= 0.10189 train_AD= 0.20222 val_MSE= 0.11508 val_AD= 0.20091 time= 5.31887\n",
      "Epoch: 0322 train_loss= 0.10060 train_MSE= 0.10060 train_AD= 0.19970 val_MSE= 0.11499 val_AD= 0.20089 time= 5.33284\n",
      "Epoch: 0323 train_loss= 0.10231 train_MSE= 0.10231 train_AD= 0.20221 val_MSE= 0.11491 val_AD= 0.20087 time= 5.34580\n",
      "Epoch: 0324 train_loss= 0.10175 train_MSE= 0.10175 train_AD= 0.20183 val_MSE= 0.11486 val_AD= 0.20085 time= 5.35877\n",
      "Epoch: 0325 train_loss= 0.09640 train_MSE= 0.09640 train_AD= 0.19796 val_MSE= 0.11476 val_AD= 0.20083 time= 5.37273\n",
      "Epoch: 0326 train_loss= 0.10158 train_MSE= 0.10158 train_AD= 0.20241 val_MSE= 0.11466 val_AD= 0.20082 time= 5.38569\n",
      "Epoch: 0327 train_loss= 0.10248 train_MSE= 0.10248 train_AD= 0.20258 val_MSE= 0.11459 val_AD= 0.20086 time= 5.40165\n",
      "Epoch: 0328 train_loss= 0.10159 train_MSE= 0.10159 train_AD= 0.20190 val_MSE= 0.11453 val_AD= 0.20088 time= 5.41661\n",
      "Epoch: 0329 train_loss= 0.10274 train_MSE= 0.10274 train_AD= 0.20352 val_MSE= 0.11453 val_AD= 0.20096 time= 5.43057\n",
      "Epoch: 0330 train_loss= 0.10292 train_MSE= 0.10292 train_AD= 0.20360 val_MSE= 0.11456 val_AD= 0.20107 time= 5.44553\n",
      "-------test_MSE= 0.11060 test_AD= 0.19783\n",
      "Epoch: 0331 train_loss= 0.10147 train_MSE= 0.10147 train_AD= 0.20169 val_MSE= 0.11461 val_AD= 0.20119 time= 5.46547\n",
      "Epoch: 0332 train_loss= 0.10277 train_MSE= 0.10277 train_AD= 0.20402 val_MSE= 0.11466 val_AD= 0.20131 time= 5.48143\n",
      "Epoch: 0333 train_loss= 0.10087 train_MSE= 0.10087 train_AD= 0.20113 val_MSE= 0.11473 val_AD= 0.20139 time= 5.49540\n",
      "Epoch: 0334 train_loss= 0.10257 train_MSE= 0.10257 train_AD= 0.20383 val_MSE= 0.11477 val_AD= 0.20146 time= 5.50836\n",
      "Epoch: 0335 train_loss= 0.09741 train_MSE= 0.09741 train_AD= 0.19760 val_MSE= 0.11476 val_AD= 0.20141 time= 5.52232\n",
      "Epoch: 0336 train_loss= 0.10115 train_MSE= 0.10115 train_AD= 0.20172 val_MSE= 0.11474 val_AD= 0.20134 time= 5.53429\n",
      "Epoch: 0337 train_loss= 0.10196 train_MSE= 0.10196 train_AD= 0.20222 val_MSE= 0.11472 val_AD= 0.20129 time= 5.54825\n",
      "Epoch: 0338 train_loss= 0.09506 train_MSE= 0.09506 train_AD= 0.20103 val_MSE= 0.11468 val_AD= 0.20127 time= 5.56022\n",
      "Epoch: 0339 train_loss= 0.10237 train_MSE= 0.10237 train_AD= 0.20341 val_MSE= 0.11466 val_AD= 0.20129 time= 5.57320\n",
      "Epoch: 0340 train_loss= 0.10111 train_MSE= 0.10111 train_AD= 0.20219 val_MSE= 0.11465 val_AD= 0.20134 time= 5.58615\n",
      "-------test_MSE= 0.11071 test_AD= 0.19844\n",
      "Epoch: 0341 train_loss= 0.10285 train_MSE= 0.10285 train_AD= 0.20440 val_MSE= 0.11468 val_AD= 0.20144 time= 5.60410\n",
      "Epoch: 0342 train_loss= 0.10212 train_MSE= 0.10212 train_AD= 0.20277 val_MSE= 0.11472 val_AD= 0.20158 time= 5.61707\n",
      "Epoch: 0343 train_loss= 0.08415 train_MSE= 0.08415 train_AD= 0.19289 val_MSE= 0.11478 val_AD= 0.20167 time= 5.63303\n",
      "Epoch: 0344 train_loss= 0.09350 train_MSE= 0.09350 train_AD= 0.19448 val_MSE= 0.11488 val_AD= 0.20179 time= 5.64699\n",
      "Epoch: 0345 train_loss= 0.10121 train_MSE= 0.10121 train_AD= 0.20228 val_MSE= 0.11493 val_AD= 0.20188 time= 5.66095\n",
      "Epoch: 0346 train_loss= 0.10265 train_MSE= 0.10265 train_AD= 0.20421 val_MSE= 0.11493 val_AD= 0.20193 time= 5.67392\n",
      "Epoch: 0347 train_loss= 0.10205 train_MSE= 0.10205 train_AD= 0.20301 val_MSE= 0.11492 val_AD= 0.20196 time= 5.68688\n",
      "Epoch: 0348 train_loss= 0.10144 train_MSE= 0.10144 train_AD= 0.20187 val_MSE= 0.11489 val_AD= 0.20197 time= 5.70084\n",
      "Epoch: 0349 train_loss= 0.09965 train_MSE= 0.09965 train_AD= 0.19977 val_MSE= 0.11485 val_AD= 0.20196 time= 5.71381\n",
      "Epoch: 0350 train_loss= 0.10175 train_MSE= 0.10175 train_AD= 0.20273 val_MSE= 0.11479 val_AD= 0.20193 time= 5.72578\n",
      "-------test_MSE= 0.11084 test_AD= 0.19904\n",
      "Epoch: 0351 train_loss= 0.10229 train_MSE= 0.10229 train_AD= 0.20396 val_MSE= 0.11473 val_AD= 0.20191 time= 5.74273\n",
      "Epoch: 0352 train_loss= 0.10141 train_MSE= 0.10141 train_AD= 0.20314 val_MSE= 0.11467 val_AD= 0.20186 time= 5.75473\n",
      "Epoch: 0353 train_loss= 0.10001 train_MSE= 0.10001 train_AD= 0.20112 val_MSE= 0.11461 val_AD= 0.20183 time= 5.76867\n",
      "Epoch: 0354 train_loss= 0.09945 train_MSE= 0.09945 train_AD= 0.20087 val_MSE= 0.11452 val_AD= 0.20183 time= 5.78063\n",
      "Epoch: 0355 train_loss= 0.10171 train_MSE= 0.10171 train_AD= 0.20253 val_MSE= 0.11443 val_AD= 0.20185 time= 5.79260\n",
      "Epoch: 0356 train_loss= 0.09998 train_MSE= 0.09998 train_AD= 0.20018 val_MSE= 0.11435 val_AD= 0.20187 time= 5.80357\n",
      "Epoch: 0357 train_loss= 0.09983 train_MSE= 0.09983 train_AD= 0.19908 val_MSE= 0.11426 val_AD= 0.20185 time= 5.81755\n",
      "Epoch: 0358 train_loss= 0.10070 train_MSE= 0.10070 train_AD= 0.20096 val_MSE= 0.11419 val_AD= 0.20183 time= 5.83449\n",
      "Epoch: 0359 train_loss= 0.09568 train_MSE= 0.09568 train_AD= 0.19770 val_MSE= 0.11410 val_AD= 0.20181 time= 5.84645\n",
      "Epoch: 0360 train_loss= 0.10061 train_MSE= 0.10061 train_AD= 0.20184 val_MSE= 0.11400 val_AD= 0.20181 time= 5.85743\n",
      "-------test_MSE= 0.11086 test_AD= 0.19932\n",
      "Epoch: 0361 train_loss= 0.10173 train_MSE= 0.10173 train_AD= 0.20236 val_MSE= 0.11392 val_AD= 0.20186 time= 5.87338\n",
      "Epoch: 0362 train_loss= 0.10113 train_MSE= 0.10113 train_AD= 0.20202 val_MSE= 0.11386 val_AD= 0.20191 time= 5.88737\n",
      "Epoch: 0363 train_loss= 0.10107 train_MSE= 0.10107 train_AD= 0.20181 val_MSE= 0.11385 val_AD= 0.20199 time= 5.90031\n",
      "Epoch: 0364 train_loss= 0.10189 train_MSE= 0.10189 train_AD= 0.20291 val_MSE= 0.11387 val_AD= 0.20211 time= 5.91228\n",
      "Epoch: 0365 train_loss= 0.10061 train_MSE= 0.10061 train_AD= 0.20154 val_MSE= 0.11392 val_AD= 0.20225 time= 5.92525\n",
      "Epoch: 0366 train_loss= 0.10163 train_MSE= 0.10163 train_AD= 0.20297 val_MSE= 0.11398 val_AD= 0.20237 time= 5.93821\n",
      "Epoch: 0367 train_loss= 0.10091 train_MSE= 0.10091 train_AD= 0.20210 val_MSE= 0.11404 val_AD= 0.20246 time= 5.95018\n",
      "Epoch: 0368 train_loss= 0.10069 train_MSE= 0.10069 train_AD= 0.20195 val_MSE= 0.11415 val_AD= 0.20255 time= 5.96115\n",
      "Epoch: 0369 train_loss= 0.09707 train_MSE= 0.09707 train_AD= 0.19817 val_MSE= 0.11419 val_AD= 0.20251 time= 5.97411\n",
      "Epoch: 0370 train_loss= 0.09978 train_MSE= 0.09978 train_AD= 0.20060 val_MSE= 0.11423 val_AD= 0.20247 time= 5.98808\n",
      "-------test_MSE= 0.11100 test_AD= 0.20006\n",
      "Epoch: 0371 train_loss= 0.10104 train_MSE= 0.10104 train_AD= 0.20150 val_MSE= 0.11424 val_AD= 0.20242 time= 6.00803\n",
      "Epoch: 0372 train_loss= 0.10154 train_MSE= 0.10154 train_AD= 0.20302 val_MSE= 0.11427 val_AD= 0.20244 time= 6.02000\n",
      "Epoch: 0373 train_loss= 0.09394 train_MSE= 0.09394 train_AD= 0.20024 val_MSE= 0.11428 val_AD= 0.20246 time= 6.03196\n",
      "Epoch: 0374 train_loss= 0.10120 train_MSE= 0.10120 train_AD= 0.20262 val_MSE= 0.11430 val_AD= 0.20251 time= 6.04293\n",
      "Epoch: 0375 train_loss= 0.10076 train_MSE= 0.10076 train_AD= 0.20267 val_MSE= 0.11435 val_AD= 0.20262 time= 6.05589\n",
      "Epoch: 0376 train_loss= 0.10136 train_MSE= 0.10136 train_AD= 0.20270 val_MSE= 0.11443 val_AD= 0.20276 time= 6.06889\n",
      "Epoch: 0377 train_loss= 0.08439 train_MSE= 0.08439 train_AD= 0.19384 val_MSE= 0.11451 val_AD= 0.20288 time= 6.07983\n",
      "Epoch: 0378 train_loss= 0.09343 train_MSE= 0.09343 train_AD= 0.19483 val_MSE= 0.11463 val_AD= 0.20297 time= 6.09280\n",
      "Epoch: 0379 train_loss= 0.09811 train_MSE= 0.09811 train_AD= 0.19915 val_MSE= 0.11468 val_AD= 0.20306 time= 6.10477\n",
      "Epoch: 0380 train_loss= 0.10157 train_MSE= 0.10157 train_AD= 0.20354 val_MSE= 0.11467 val_AD= 0.20312 time= 6.11675\n",
      "-------test_MSE= 0.11113 test_AD= 0.20063\n",
      "Epoch: 0381 train_loss= 0.10161 train_MSE= 0.10161 train_AD= 0.20363 val_MSE= 0.11463 val_AD= 0.20314 time= 6.13369\n",
      "Epoch: 0382 train_loss= 0.10072 train_MSE= 0.10072 train_AD= 0.20144 val_MSE= 0.11460 val_AD= 0.20314 time= 6.14468\n",
      "Epoch: 0383 train_loss= 0.10036 train_MSE= 0.10036 train_AD= 0.20162 val_MSE= 0.11455 val_AD= 0.20311 time= 6.15363\n",
      "Epoch: 0384 train_loss= 0.09870 train_MSE= 0.09870 train_AD= 0.19933 val_MSE= 0.11448 val_AD= 0.20310 time= 6.16361\n",
      "Epoch: 0385 train_loss= 0.10100 train_MSE= 0.10100 train_AD= 0.20264 val_MSE= 0.11441 val_AD= 0.20307 time= 6.17458\n",
      "Epoch: 0386 train_loss= 0.10063 train_MSE= 0.10063 train_AD= 0.20332 val_MSE= 0.11436 val_AD= 0.20303 time= 6.18558\n",
      "Epoch: 0387 train_loss= 0.10000 train_MSE= 0.10000 train_AD= 0.20168 val_MSE= 0.11432 val_AD= 0.20303 time= 6.19752\n",
      "Epoch: 0388 train_loss= 0.09894 train_MSE= 0.09894 train_AD= 0.20113 val_MSE= 0.11427 val_AD= 0.20305 time= 6.20849\n",
      "Epoch: 0389 train_loss= 0.09920 train_MSE= 0.09920 train_AD= 0.19981 val_MSE= 0.11420 val_AD= 0.20307 time= 6.21846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0390 train_loss= 0.09985 train_MSE= 0.09985 train_AD= 0.20139 val_MSE= 0.11414 val_AD= 0.20311 time= 6.23043\n",
      "-------test_MSE= 0.11116 test_AD= 0.20090\n",
      "Epoch: 0391 train_loss= 0.09827 train_MSE= 0.09827 train_AD= 0.19775 val_MSE= 0.11408 val_AD= 0.20309 time= 6.24838\n",
      "Epoch: 0392 train_loss= 0.10038 train_MSE= 0.10038 train_AD= 0.20135 val_MSE= 0.11404 val_AD= 0.20308 time= 6.26135\n",
      "Epoch: 0393 train_loss= 0.10043 train_MSE= 0.10043 train_AD= 0.20177 val_MSE= 0.11403 val_AD= 0.20310 time= 6.27232\n",
      "Epoch: 0394 train_loss= 0.09473 train_MSE= 0.09473 train_AD= 0.19750 val_MSE= 0.11396 val_AD= 0.20310 time= 6.28829\n",
      "Epoch: 0395 train_loss= 0.09952 train_MSE= 0.09952 train_AD= 0.20064 val_MSE= 0.11390 val_AD= 0.20315 time= 6.30224\n",
      "Epoch: 0396 train_loss= 0.10080 train_MSE= 0.10080 train_AD= 0.20234 val_MSE= 0.11386 val_AD= 0.20322 time= 6.31520\n",
      "Epoch: 0397 train_loss= 0.09978 train_MSE= 0.09978 train_AD= 0.20137 val_MSE= 0.11383 val_AD= 0.20326 time= 6.32917\n",
      "Epoch: 0398 train_loss= 0.10069 train_MSE= 0.10069 train_AD= 0.20232 val_MSE= 0.11385 val_AD= 0.20336 time= 6.34113\n",
      "Epoch: 0399 train_loss= 0.10084 train_MSE= 0.10084 train_AD= 0.20251 val_MSE= 0.11391 val_AD= 0.20348 time= 6.35210\n",
      "Epoch: 0400 train_loss= 0.09967 train_MSE= 0.09967 train_AD= 0.20149 val_MSE= 0.11400 val_AD= 0.20361 time= 6.36407\n",
      "-------test_MSE= 0.11125 test_AD= 0.20153\n",
      "Epoch: 0401 train_loss= 0.10060 train_MSE= 0.10060 train_AD= 0.20236 val_MSE= 0.11407 val_AD= 0.20371 time= 6.38103\n",
      "Epoch: 0402 train_loss= 0.09855 train_MSE= 0.09855 train_AD= 0.19987 val_MSE= 0.11416 val_AD= 0.20379 time= 6.39400\n",
      "Epoch: 0403 train_loss= 0.09957 train_MSE= 0.09957 train_AD= 0.20143 val_MSE= 0.11417 val_AD= 0.20373 time= 6.40399\n",
      "Epoch: 0404 train_loss= 0.09684 train_MSE= 0.09684 train_AD= 0.19837 val_MSE= 0.11419 val_AD= 0.20368 time= 6.41593\n",
      "Epoch: 0405 train_loss= 0.09898 train_MSE= 0.09898 train_AD= 0.20045 val_MSE= 0.11419 val_AD= 0.20363 time= 6.42690\n",
      "Epoch: 0406 train_loss= 0.10039 train_MSE= 0.10039 train_AD= 0.20213 val_MSE= 0.11421 val_AD= 0.20364 time= 6.43887\n",
      "Epoch: 0407 train_loss= 0.09313 train_MSE= 0.09313 train_AD= 0.19974 val_MSE= 0.11422 val_AD= 0.20366 time= 6.45184\n",
      "Epoch: 0408 train_loss= 0.10009 train_MSE= 0.10009 train_AD= 0.20207 val_MSE= 0.11424 val_AD= 0.20370 time= 6.46181\n",
      "Epoch: 0409 train_loss= 0.09985 train_MSE= 0.09985 train_AD= 0.20255 val_MSE= 0.11429 val_AD= 0.20381 time= 6.47278\n",
      "Epoch: 0410 train_loss= 0.10045 train_MSE= 0.10045 train_AD= 0.20284 val_MSE= 0.11437 val_AD= 0.20394 time= 6.48575\n",
      "-------test_MSE= 0.11137 test_AD= 0.20203\n",
      "Epoch: 0411 train_loss= 0.08411 train_MSE= 0.08411 train_AD= 0.19475 val_MSE= 0.11447 val_AD= 0.20407 time= 6.50270\n",
      "Epoch: 0412 train_loss= 0.09774 train_MSE= 0.09774 train_AD= 0.19841 val_MSE= 0.11457 val_AD= 0.20415 time= 6.51368\n",
      "Epoch: 0413 train_loss= 0.09250 train_MSE= 0.09250 train_AD= 0.19509 val_MSE= 0.11464 val_AD= 0.20424 time= 6.52664\n",
      "Epoch: 0414 train_loss= 0.09950 train_MSE= 0.09950 train_AD= 0.20215 val_MSE= 0.11464 val_AD= 0.20429 time= 6.54062\n",
      "Epoch: 0415 train_loss= 0.10052 train_MSE= 0.10052 train_AD= 0.20311 val_MSE= 0.11461 val_AD= 0.20431 time= 6.55157\n",
      "Epoch: 0416 train_loss= 0.10008 train_MSE= 0.10008 train_AD= 0.20210 val_MSE= 0.11463 val_AD= 0.20432 time= 6.56354\n",
      "Epoch: 0417 train_loss= 0.09914 train_MSE= 0.09914 train_AD= 0.20066 val_MSE= 0.11461 val_AD= 0.20428 time= 6.57551\n",
      "Epoch: 0418 train_loss= 0.09779 train_MSE= 0.09779 train_AD= 0.19933 val_MSE= 0.11460 val_AD= 0.20428 time= 6.58849\n",
      "Epoch: 0419 train_loss= 0.10020 train_MSE= 0.10020 train_AD= 0.20283 val_MSE= 0.11459 val_AD= 0.20428 time= 6.59947\n",
      "Epoch: 0420 train_loss= 0.10039 train_MSE= 0.10039 train_AD= 0.20331 val_MSE= 0.11457 val_AD= 0.20427 time= 6.61142\n",
      "-------test_MSE= 0.11139 test_AD= 0.20246\n",
      "Epoch: 0421 train_loss= 0.09807 train_MSE= 0.09807 train_AD= 0.20028 val_MSE= 0.11458 val_AD= 0.20427 time= 6.62839\n",
      "Epoch: 0422 train_loss= 0.09939 train_MSE= 0.09939 train_AD= 0.20214 val_MSE= 0.11460 val_AD= 0.20430 time= 6.64233\n",
      "Epoch: 0423 train_loss= 0.09735 train_MSE= 0.09735 train_AD= 0.19993 val_MSE= 0.11458 val_AD= 0.20432 time= 6.65530\n",
      "Epoch: 0424 train_loss= 0.09904 train_MSE= 0.09904 train_AD= 0.20103 val_MSE= 0.11455 val_AD= 0.20437 time= 6.67125\n",
      "Epoch: 0425 train_loss= 0.09769 train_MSE= 0.09769 train_AD= 0.19851 val_MSE= 0.11451 val_AD= 0.20435 time= 6.68222\n",
      "Epoch: 0426 train_loss= 0.09937 train_MSE= 0.09937 train_AD= 0.20088 val_MSE= 0.11449 val_AD= 0.20432 time= 6.69319\n",
      "Epoch: 0427 train_loss= 0.09884 train_MSE= 0.09884 train_AD= 0.20062 val_MSE= 0.11450 val_AD= 0.20432 time= 6.70316\n",
      "Epoch: 0428 train_loss= 0.09392 train_MSE= 0.09392 train_AD= 0.19709 val_MSE= 0.11444 val_AD= 0.20429 time= 6.71515\n",
      "Epoch: 0429 train_loss= 0.09880 train_MSE= 0.09880 train_AD= 0.20132 val_MSE= 0.11437 val_AD= 0.20428 time= 6.72411\n",
      "Epoch: 0430 train_loss= 0.09967 train_MSE= 0.09967 train_AD= 0.20156 val_MSE= 0.11434 val_AD= 0.20434 time= 6.73511\n",
      "-------test_MSE= 0.11147 test_AD= 0.20281\n",
      "Epoch: 0431 train_loss= 0.09867 train_MSE= 0.09867 train_AD= 0.20074 val_MSE= 0.11432 val_AD= 0.20437 time= 6.75303\n",
      "Epoch: 0432 train_loss= 0.09979 train_MSE= 0.09979 train_AD= 0.20237 val_MSE= 0.11436 val_AD= 0.20448 time= 6.76700\n",
      "Epoch: 0433 train_loss= 0.09998 train_MSE= 0.09998 train_AD= 0.20259 val_MSE= 0.11443 val_AD= 0.20462 time= 6.77896\n",
      "Epoch: 0434 train_loss= 0.09846 train_MSE= 0.09846 train_AD= 0.20050 val_MSE= 0.11453 val_AD= 0.20477 time= 6.79193\n",
      "Epoch: 0435 train_loss= 0.09983 train_MSE= 0.09983 train_AD= 0.20306 val_MSE= 0.11462 val_AD= 0.20491 time= 6.80790\n",
      "Epoch: 0436 train_loss= 0.09786 train_MSE= 0.09786 train_AD= 0.19992 val_MSE= 0.11473 val_AD= 0.20501 time= 6.82185\n",
      "Epoch: 0437 train_loss= 0.09964 train_MSE= 0.09964 train_AD= 0.20284 val_MSE= 0.11478 val_AD= 0.20508 time= 6.83482\n",
      "Epoch: 0438 train_loss= 0.09504 train_MSE= 0.09504 train_AD= 0.19704 val_MSE= 0.11476 val_AD= 0.20501 time= 6.84678\n",
      "Epoch: 0439 train_loss= 0.09831 train_MSE= 0.09831 train_AD= 0.20091 val_MSE= 0.11473 val_AD= 0.20491 time= 6.85775\n",
      "Epoch: 0440 train_loss= 0.09894 train_MSE= 0.09894 train_AD= 0.20107 val_MSE= 0.11471 val_AD= 0.20484 time= 6.86772\n",
      "-------test_MSE= 0.11159 test_AD= 0.20342\n",
      "Epoch: 0441 train_loss= 0.09264 train_MSE= 0.09264 train_AD= 0.20025 val_MSE= 0.11468 val_AD= 0.20479 time= 6.88468\n",
      "Epoch: 0442 train_loss= 0.09942 train_MSE= 0.09942 train_AD= 0.20240 val_MSE= 0.11468 val_AD= 0.20480 time= 6.89665\n",
      "Epoch: 0443 train_loss= 0.09815 train_MSE= 0.09815 train_AD= 0.20099 val_MSE= 0.11470 val_AD= 0.20486 time= 6.91061\n",
      "Epoch: 0444 train_loss= 0.09988 train_MSE= 0.09988 train_AD= 0.20335 val_MSE= 0.11479 val_AD= 0.20499 time= 6.92458\n",
      "Epoch: 0445 train_loss= 0.09919 train_MSE= 0.09919 train_AD= 0.20189 val_MSE= 0.11489 val_AD= 0.20519 time= 6.93654\n",
      "Epoch: 0446 train_loss= 0.08118 train_MSE= 0.08118 train_AD= 0.19178 val_MSE= 0.11502 val_AD= 0.20533 time= 6.94752\n",
      "Epoch: 0447 train_loss= 0.09094 train_MSE= 0.09094 train_AD= 0.19391 val_MSE= 0.11511 val_AD= 0.20545 time= 6.95748\n",
      "Epoch: 0448 train_loss= 0.09853 train_MSE= 0.09853 train_AD= 0.20162 val_MSE= 0.11514 val_AD= 0.20552 time= 6.96746\n",
      "Epoch: 0449 train_loss= 0.09969 train_MSE= 0.09969 train_AD= 0.20329 val_MSE= 0.11509 val_AD= 0.20551 time= 6.97643\n",
      "Epoch: 0450 train_loss= 0.09921 train_MSE= 0.09921 train_AD= 0.20234 val_MSE= 0.11501 val_AD= 0.20546 time= 6.98644\n",
      "-------test_MSE= 0.11176 test_AD= 0.20390\n",
      "Epoch: 0451 train_loss= 0.09862 train_MSE= 0.09862 train_AD= 0.20133 val_MSE= 0.11492 val_AD= 0.20539 time= 7.00138\n",
      "Epoch: 0452 train_loss= 0.09695 train_MSE= 0.09695 train_AD= 0.19917 val_MSE= 0.11483 val_AD= 0.20540 time= 7.01434\n",
      "Epoch: 0453 train_loss= 0.09881 train_MSE= 0.09881 train_AD= 0.20183 val_MSE= 0.11474 val_AD= 0.20539 time= 7.02430\n",
      "Epoch: 0454 train_loss= 0.09938 train_MSE= 0.09938 train_AD= 0.20314 val_MSE= 0.11468 val_AD= 0.20534 time= 7.03627\n",
      "Epoch: 0455 train_loss= 0.09870 train_MSE= 0.09870 train_AD= 0.20258 val_MSE= 0.11465 val_AD= 0.20527 time= 7.04824\n",
      "Epoch: 0456 train_loss= 0.09724 train_MSE= 0.09724 train_AD= 0.20040 val_MSE= 0.11464 val_AD= 0.20522 time= 7.06021\n",
      "Epoch: 0457 train_loss= 0.09676 train_MSE= 0.09676 train_AD= 0.20014 val_MSE= 0.11462 val_AD= 0.20526 time= 7.07118\n",
      "Epoch: 0458 train_loss= 0.09878 train_MSE= 0.09878 train_AD= 0.20165 val_MSE= 0.11459 val_AD= 0.20530 time= 7.08215\n",
      "Epoch: 0459 train_loss= 0.09725 train_MSE= 0.09725 train_AD= 0.19961 val_MSE= 0.11458 val_AD= 0.20535 time= 7.09413\n",
      "Epoch: 0460 train_loss= 0.09699 train_MSE= 0.09699 train_AD= 0.19838 val_MSE= 0.11455 val_AD= 0.20532 time= 7.10808\n",
      "-------test_MSE= 0.11183 test_AD= 0.20424\n",
      "Epoch: 0461 train_loss= 0.09779 train_MSE= 0.09779 train_AD= 0.20009 val_MSE= 0.11453 val_AD= 0.20531 time= 7.12404\n",
      "Epoch: 0462 train_loss= 0.09330 train_MSE= 0.09330 train_AD= 0.19727 val_MSE= 0.11448 val_AD= 0.20527 time= 7.13401\n",
      "Epoch: 0463 train_loss= 0.09786 train_MSE= 0.09786 train_AD= 0.20126 val_MSE= 0.11441 val_AD= 0.20522 time= 7.14498\n",
      "Epoch: 0464 train_loss= 0.09899 train_MSE= 0.09899 train_AD= 0.20183 val_MSE= 0.11437 val_AD= 0.20524 time= 7.15795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0465 train_loss= 0.09828 train_MSE= 0.09828 train_AD= 0.20130 val_MSE= 0.11434 val_AD= 0.20527 time= 7.17091\n",
      "Epoch: 0466 train_loss= 0.09822 train_MSE= 0.09822 train_AD= 0.20122 val_MSE= 0.11439 val_AD= 0.20538 time= 7.18388\n",
      "Epoch: 0467 train_loss= 0.09893 train_MSE= 0.09893 train_AD= 0.20222 val_MSE= 0.11447 val_AD= 0.20552 time= 7.19684\n",
      "Epoch: 0468 train_loss= 0.09772 train_MSE= 0.09772 train_AD= 0.20092 val_MSE= 0.11459 val_AD= 0.20570 time= 7.21280\n",
      "Epoch: 0469 train_loss= 0.09877 train_MSE= 0.09877 train_AD= 0.20247 val_MSE= 0.11470 val_AD= 0.20587 time= 7.22577\n",
      "Epoch: 0470 train_loss= 0.09803 train_MSE= 0.09803 train_AD= 0.20154 val_MSE= 0.11479 val_AD= 0.20599 time= 7.23874\n",
      "-------test_MSE= 0.11194 test_AD= 0.20489\n",
      "Epoch: 0471 train_loss= 0.09774 train_MSE= 0.09774 train_AD= 0.20124 val_MSE= 0.11489 val_AD= 0.20610 time= 7.25372\n",
      "Epoch: 0472 train_loss= 0.09479 train_MSE= 0.09479 train_AD= 0.19800 val_MSE= 0.11489 val_AD= 0.20605 time= 7.26466\n",
      "Epoch: 0473 train_loss= 0.09699 train_MSE= 0.09699 train_AD= 0.20017 val_MSE= 0.11489 val_AD= 0.20599 time= 7.27464\n",
      "Epoch: 0474 train_loss= 0.09814 train_MSE= 0.09814 train_AD= 0.20090 val_MSE= 0.11487 val_AD= 0.20593 time= 7.28361\n",
      "Epoch: 0475 train_loss= 0.09871 train_MSE= 0.09871 train_AD= 0.20255 val_MSE= 0.11490 val_AD= 0.20596 time= 7.29458\n",
      "Epoch: 0476 train_loss= 0.09157 train_MSE= 0.09157 train_AD= 0.19972 val_MSE= 0.11491 val_AD= 0.20599 time= 7.30655\n",
      "Epoch: 0477 train_loss= 0.09830 train_MSE= 0.09830 train_AD= 0.20174 val_MSE= 0.11494 val_AD= 0.20606 time= 7.31755\n",
      "Epoch: 0478 train_loss= 0.09789 train_MSE= 0.09789 train_AD= 0.20207 val_MSE= 0.11503 val_AD= 0.20622 time= 7.32752\n",
      "Epoch: 0479 train_loss= 0.09844 train_MSE= 0.09844 train_AD= 0.20208 val_MSE= 0.11515 val_AD= 0.20643 time= 7.33846\n",
      "Epoch: 0480 train_loss= 0.08151 train_MSE= 0.08151 train_AD= 0.19325 val_MSE= 0.11527 val_AD= 0.20660 time= 7.34844\n",
      "-------test_MSE= 0.11213 test_AD= 0.20529\n",
      "Epoch: 0481 train_loss= 0.09074 train_MSE= 0.09074 train_AD= 0.19449 val_MSE= 0.11542 val_AD= 0.20674 time= 7.36440\n",
      "Epoch: 0482 train_loss= 0.09569 train_MSE= 0.09569 train_AD= 0.19907 val_MSE= 0.11546 val_AD= 0.20679 time= 7.37836\n",
      "Epoch: 0483 train_loss= 0.09874 train_MSE= 0.09874 train_AD= 0.20310 val_MSE= 0.11544 val_AD= 0.20676 time= 7.39132\n",
      "Epoch: 0484 train_loss= 0.09877 train_MSE= 0.09877 train_AD= 0.20313 val_MSE= 0.11538 val_AD= 0.20674 time= 7.40429\n",
      "Epoch: 0485 train_loss= 0.09807 train_MSE= 0.09807 train_AD= 0.20147 val_MSE= 0.11532 val_AD= 0.20670 time= 7.41526\n",
      "Epoch: 0486 train_loss= 0.09770 train_MSE= 0.09770 train_AD= 0.20131 val_MSE= 0.11525 val_AD= 0.20664 time= 7.42823\n",
      "Epoch: 0487 train_loss= 0.09592 train_MSE= 0.09592 train_AD= 0.19886 val_MSE= 0.11519 val_AD= 0.20661 time= 7.43919\n",
      "Epoch: 0488 train_loss= 0.09821 train_MSE= 0.09821 train_AD= 0.20220 val_MSE= 0.11515 val_AD= 0.20656 time= 7.45217\n",
      "Epoch: 0489 train_loss= 0.09800 train_MSE= 0.09800 train_AD= 0.20294 val_MSE= 0.11514 val_AD= 0.20652 time= 7.46513\n",
      "Epoch: 0490 train_loss= 0.09729 train_MSE= 0.09729 train_AD= 0.20125 val_MSE= 0.11516 val_AD= 0.20651 time= 7.47709\n",
      "-------test_MSE= 0.11212 test_AD= 0.20556\n",
      "Epoch: 0491 train_loss= 0.09617 train_MSE= 0.09617 train_AD= 0.20063 val_MSE= 0.11520 val_AD= 0.20650 time= 7.49305\n",
      "Epoch: 0492 train_loss= 0.09664 train_MSE= 0.09664 train_AD= 0.19953 val_MSE= 0.11519 val_AD= 0.20645 time= 7.50502\n",
      "Epoch: 0493 train_loss= 0.09720 train_MSE= 0.09720 train_AD= 0.20115 val_MSE= 0.11519 val_AD= 0.20649 time= 7.51499\n",
      "Epoch: 0494 train_loss= 0.09561 train_MSE= 0.09561 train_AD= 0.19753 val_MSE= 0.11517 val_AD= 0.20645 time= 7.52696\n",
      "Epoch: 0495 train_loss= 0.09756 train_MSE= 0.09756 train_AD= 0.20074 val_MSE= 0.11515 val_AD= 0.20642 time= 7.53594\n",
      "Epoch: 0496 train_loss= 0.09767 train_MSE= 0.09767 train_AD= 0.20135 val_MSE= 0.11518 val_AD= 0.20644 time= 7.54491\n",
      "Epoch: 0497 train_loss= 0.09249 train_MSE= 0.09249 train_AD= 0.19748 val_MSE= 0.11512 val_AD= 0.20640 time= 7.55588\n",
      "Epoch: 0498 train_loss= 0.09696 train_MSE= 0.09696 train_AD= 0.20042 val_MSE= 0.11506 val_AD= 0.20642 time= 7.57084\n",
      "Epoch: 0499 train_loss= 0.09806 train_MSE= 0.09806 train_AD= 0.20194 val_MSE= 0.11504 val_AD= 0.20649 time= 7.58181\n",
      "Epoch: 0500 train_loss= 0.09710 train_MSE= 0.09710 train_AD= 0.20112 val_MSE= 0.11503 val_AD= 0.20653 time= 7.59279\n",
      "-------test_MSE= 0.11218 test_AD= 0.20599\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    batch_samples = get_batch_idx(epoch)\n",
    "    feed_dict.update({phs['sample_idx']: batch_samples})\n",
    "    _, train_MSE, train_AD = sess.run((opt_op, train_mse, train_absolute_diff), feed_dict=feed_dict)\n",
    "    val_MSE, val_AD = sess.run((val_mse, val_absolute_diff), \n",
    "                                         feed_dict=feed_dict_val) \n",
    "    \n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "      \"train_loss=\", \"{:.5f}\".format(train_MSE),\n",
    "      \"train_MSE=\", \"{:.5f}\".format(train_MSE),\n",
    "      \"train_AD=\", \"{:.5f}\".format(train_AD),  # AD means the absolute difference\n",
    "      \"val_MSE=\", \"{:.5f}\".format(val_MSE),\n",
    "      \"val_AD=\", \"{:.5f}\".format(val_AD),\n",
    "      \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    \n",
    "    if (epoch+1) % save_step == 0:\n",
    "        test_MSE, test_AD = sess.run((test_mse, test_absolute_diff), \n",
    "                                            feed_dict=feed_dict_val) \n",
    "        print(\"-------test_MSE=\", \"{:.5f}\".format(test_MSE),\n",
    "          \"test_AD=\", \"{:.5f}\".format(test_AD))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
